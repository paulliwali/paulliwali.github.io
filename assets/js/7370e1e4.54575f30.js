"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[38857],{28453:(e,n,s)=>{s.d(n,{R:()=>l,x:()=>r});var i=s(96540);const t={},a=i.createContext(t);function l(e){const n=i.useContext(a);return i.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:l(e.components),i.createElement(a.Provider,{value:n},e.children)}},76334:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>o,contentTitle:()=>r,default:()=>h,frontMatter:()=>l,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"slip-box/reference-notes/Practical SQL for Data Analysis","title":"\ud83d\udcf0 Summary (use your own words)","description":"SQL code for fast data analysis so one can skip using Pandas if not needed. This article is showcasing PostgreSQL.","source":"@site/docs/slip-box/reference-notes/Practical SQL for Data Analysis.md","sourceDirName":"slip-box/reference-notes","slug":"/slip-box/reference-notes/Practical SQL for Data Analysis","permalink":"/docs/slip-box/reference-notes/Practical SQL for Data Analysis","draft":false,"unlisted":false,"editUrl":"https://github.com/paulliwali/paulliwali.github.io/tree/main/docs/docs/slip-box/reference-notes/Practical SQL for Data Analysis.md","tags":[{"inline":true,"label":"sql","permalink":"/docs/tags/sql"},{"inline":true,"label":"data-science","permalink":"/docs/tags/data-science"}],"version":"current","frontMatter":{"source":"https://hakibenita.com/sql-for-data-analysis","author":"Haki Benita","tags":["sql","data-science"]},"sidebar":"tutorialSidebar","previous":{"title":"Polynomial regression model","permalink":"/docs/slip-box/reference-notes/Polynomial regression model"},"next":{"title":"Primer on casual inference","permalink":"/docs/slip-box/reference-notes/Primer on casual inference"}}');var t=s(74848),a=s(28453);const l={source:"https://hakibenita.com/sql-for-data-analysis",author:"Haki Benita",tags:["sql","data-science"]},r="\ud83d\udcf0 Summary (use your own words)",o={},c=[{value:"Common Table Expressions (CTE)",id:"common-table-expressions-cte",level:2},{value:"Generating Data",id:"generating-data",level:2},{value:"Sampling",id:"sampling",level:2},{value:"Data Exploration",id:"data-exploration",level:2},{value:"Subtotals",id:"subtotals",level:2},{value:"Pivot Tables",id:"pivot-tables",level:2},{value:"Cumulative Aggregation",id:"cumulative-aggregation",level:2},{value:"Statistics",id:"statistics",level:2}];function d(e){const n={a:"a",code:"code",em:"em",h1:"h1",h2:"h2",header:"header",li:"li",p:"p",pre:"pre",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,a.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"-summary-use-your-own-words",children:"\ud83d\udcf0 Summary (use your own words)"})}),"\n",(0,t.jsx)(n.p,{children:"SQL code for fast data analysis so one can skip using Pandas if not needed. This article is showcasing PostgreSQL."}),"\n",(0,t.jsx)(n.h1,{id:"\ufe0f-notes",children:"\u270d\ufe0f Notes"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["There is a high cost in loading data into Pandas, so it is often not worth it for ad-hoc analysis","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Or it is worth pre-analysis in SQL itself before fetching it into Pandas"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"common-table-expressions-cte",children:"Common Table Expressions (CTE)"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Good for recursive queries"}),"\n",(0,t.jsx)(n.li,{children:"Good for readability and modularity"}),"\n"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-sql",children:"WITH emails AS (\n\tSELECT 'PAUL.DENG@test.com' AS email\n), \nnormalized_emails AS ( \n\tSELECT lower(email) as email FROM emails\n)\nSELECT * FROM normalized_emails;\n"})}),"\n",(0,t.jsxs)(n.table,{children:[(0,t.jsx)(n.thead,{children:(0,t.jsx)(n.tr,{children:(0,t.jsx)(n.th,{style:{textAlign:"left"},children:"email"})})}),(0,t.jsx)(n.tbody,{children:(0,t.jsx)(n.tr,{children:(0,t.jsx)(n.td,{style:{textAlign:"left"},children:(0,t.jsx)(n.a,{href:"mailto:paul.deng@test.com",children:"paul.deng@test.com"})})})})]}),"\n",(0,t.jsx)(n.h2,{id:"generating-data",children:"Generating Data"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Useful for joining data to"}),"\n",(0,t.jsxs)(n.li,{children:["Can use ",(0,t.jsx)(n.code,{children:"UNION ALL"})," to concat data, ",(0,t.jsx)(n.code,{children:"VALUE LIST"})," to create constant data, ",(0,t.jsx)(n.code,{children:"UNNEST"})," to generate small 1 dimensional data from a list"]}),"\n",(0,t.jsxs)(n.li,{children:["A really useful command is to use ",(0,t.jsx)(n.code,{children:"GENERATE_SERIES"})," for large amounts of data with a fixed step and can be used for time series"]}),"\n"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-sql",children:"WITH daterange AS (\n\tSELECT *\n\tFROM generate_series(\n\t\t'2023-01-01 UTC'::timestamptz, --start\n\t\t'2023-01-02 UTC'::timestamptz, --end\n\t\tINTERVAL '1 hour' --step\n\t) WITH ORDINALITY AS t(hh, n)\n)\nSELECT * FROM daterange\n"})}),"\n",(0,t.jsx)(n.h2,{id:"sampling",children:"Sampling"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["To produce random numbers, one can use ",(0,t.jsx)(n.code,{children:"CEIL(RANDOM())"})," or ",(0,t.jsx)(n.code,{children:"FLOOR(RANDOM())"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["Don't use ",(0,t.jsx)(n.code,{children:"ROUND(RANDOM())"})," because it doesn't retain the distribution"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-sql",children:"SELECT \n\tCEIL(RANDOM() * 3) AS n,\n\tCOUNT(*)\nFROM generate_series(0, 1000)\nGROUP BY 1\n"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"One can extend this to produce random choice"}),"\n"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-sql",children:"SELECT \n\t(array['RED', 'BLUE', 'GREEN'])[CEIL(RANDOM() * 3)] AS color \nFROM generate_series(1, 5)\n"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["To sample you can use ",(0,t.jsx)(n.code,{children:"RANDOM()"})," but it is pretty inefficient"]}),"\n",(0,t.jsxs)(n.li,{children:["So PostgreSQL has two methods: ",(0,t.jsx)(n.code,{children:"system"})," and ",(0,t.jsx)(n.code,{children:"bernoulli"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"system"})," is done with blocks so it is faster"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-sql",children:"-- using random\nWITH random_sample AS ( \n\tSELECT * \n\tFROM users \n\tORDER BY RANDOM() LIMIT 1000\n)\nSELECT COUNT(*) FROM random_sample\n\n-- using system\nWITH system_sample AS (\n\tSELECT *\n\tFROM users TABLESAMPLE() SYSTEM(10) -- argument is percentage\n)\nSELECT COUNT(*) FROM system_sample\n\n-- using bernoulli \nWITH bernoulli_sample AS (\n\tSELECT *\n\tFROM users TABLESAMPLE() BERNOULLI(10) -- argument is percentage\n)\nSELECT COUNT(*) FROM bernoulli_sample\n"})}),"\n",(0,t.jsx)(n.h2,{id:"data-exploration",children:"Data Exploration"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["Can calculate simple statistical descriptions with ",(0,t.jsx)(n.code,{children:"COUNT"}),", ",(0,t.jsx)(n.code,{children:"AVG"}),", ",(0,t.jsx)(n.code,{children:"STDDEV"}),", ",(0,t.jsx)(n.code,{children:"MIN"}),", ",(0,t.jsx)(n.code,{children:"MAX"}),", ",(0,t.jsx)(n.code,{children:"MODE"})," and ",(0,t.jsx)(n.code,{children:"PERCENTILE_CONT"})," or ",(0,t.jsx)(n.code,{children:"PERCENTILE_DISC"})]}),"\n",(0,t.jsxs)(n.li,{children:["The median of a series can be calculated with either percentile functions","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY n)"})," gives ",(0,t.jsx)(n.em,{children:"a value"})," that 50% of the values are less than which could be a value that doesn't exist in the series"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"PERCENTILE_DISC(0.5) WITHIN GROUP (ORDER BY n)"})," gives ",(0,t.jsx)(n.em,{children:"the value"})," that 50% of the values are less than which is from the series"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"subtotals",children:"Subtotals"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["There is a ",(0,t.jsx)(n.code,{children:"ROLLUP"})," command that gives hierarchical subtotals, but only for the combination specified"]}),"\n"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-sql",children:"WITH emp AS (\n\tSELECT * FROM (VALUES\n\t\t('Haki', 'R&D', 'Manager'),\n\t\t('Dan', 'R&D', 'Developer'),\n\t\t('Jax', 'R&D', 'Developer'),\n\t\t('George', 'Sales', 'Manager'),\n\t\t('Bill', 'Sales', 'Developer'),\n\t\t('David', 'Sales', 'Developer')\n\t) AS t(\n\t name, department, role\n\t)\n)\nSELECT department, role, COUNT(*)\nFROM emp\nGROUP BY ROLLUP(department), role;\n"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["To get ",(0,t.jsx)(n.em,{children:"all combinations"}),", we can precalculate them using ",(0,t.jsx)(n.code,{children:"CUBE"})]}),"\n",(0,t.jsxs)(n.li,{children:["Both of these commands are just syntax sugar for ",(0,t.jsx)(n.code,{children:"grouping sets"})," which gives the explicit combinations"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"pivot-tables",children:"Pivot Tables"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["We can also reshape data in SQL by using conditional expressions ",(0,t.jsx)(n.code,{children:"CASE ... WHEN ... THEN .. WHEN ... THEN ... ELSE ... END"})]}),"\n",(0,t.jsxs)(n.li,{children:["Another way to do this is through aggregate expressions ",(0,t.jsx)(n.code,{children:"COUNT(*) FILTER (WHERE ...) AS"})]}),"\n",(0,t.jsx)(n.li,{children:"Overall, it is still tedious to do compared to pandas so if there are lots of transformations being done it might be better to use pandas here"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"cumulative-aggregation",children:"Cumulative Aggregation"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["Use the window function ",(0,t.jsx)(n.code,{children:"OVER (PARTITION BY ...)"})," and passing that into a statistical function","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"The partition by command can take an integer to evaluate over all the rows"}),"\n",(0,t.jsx)(n.li,{children:"It can also take a frame clause to perform moving window"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"statistics",children:"Statistics"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["PostgreSQL even have built in regression functions like ",(0,t.jsx)(n.code,{children:"REGR_SLOPE()"}),", ",(0,t.jsx)(n.code,{children:"REGR_INTERCEPT"})]}),"\n",(0,t.jsxs)(n.li,{children:["Interpolating missing data or finer granularity is also a common task, PostgreSQL can do this with ",(0,t.jsx)(n.code,{children:"COALESC"})," to get a default value and more complex query to do forward fill and back fill as well"]}),"\n",(0,t.jsxs)(n.li,{children:["Binning with ",(0,t.jsx)(n.code,{children:"CASE ... WHEN"})," and if we want equal height binning we can use ",(0,t.jsx)(n.code,{children:"NTILE"})," and histogram (equal width binning) with ",(0,t.jsx)(n.code,{children:"WIDTH_BUCKET"})]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}}}]);