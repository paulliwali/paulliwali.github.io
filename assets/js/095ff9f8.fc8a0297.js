"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[72038],{28453:(e,n,s)=>{s.d(n,{R:()=>a,x:()=>o});var t=s(96540);const i={},r=t.createContext(i);function a(e){const n=t.useContext(r);return t.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:a(e.components),t.createElement(r.Provider,{value:n},e.children)}},88730:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>u,frontMatter:()=>a,metadata:()=>t,toc:()=>d});const t=JSON.parse('{"id":"Technical skills with gemini","title":"Context","description":"We have a dataset representing user interactions with a new \\"Stories\\" feature. Each row in our interactions DataFrame corresponds to a single user interaction, and it has the following columns:","source":"@site/docs/Technical skills with gemini.md","sourceDirName":".","slug":"/Technical skills with gemini","permalink":"/docs/Technical skills with gemini","draft":false,"unlisted":false,"editUrl":"https://github.com/paulliwali/paulliwali.github.io/tree/main/docs/docs/Technical skills with gemini.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Finance","permalink":"/docs/Finance"},"next":{"title":"Trip to Japan","permalink":"/docs/Trip to Japan"}}');var i=s(74848),r=s(28453);const a={},o="Context",l={},d=[{value:"<strong>Question 1:</strong> How would you use pandas to calculate the total number of unique users who viewed <em>at least one</em> story?",id:"question-1-how-would-you-use-pandas-to-calculate-the-total-number-of-unique-users-who-viewed-at-least-one-story",level:3},{value:"Question 2:** We want to understand the popularity of stories. How would you use pandas to find the top 5 most viewed stories, along with the count of views for each?",id:"question-2-we-want-to-understand-the-popularity-of-stories-how-would-you-use-pandas-to-find-the-top-5-most-viewed-stories-along-with-the-count-of-views-for-each",level:2},{value:"<strong>Question 3:</strong> We are concerned about user &quot;churn&quot; on the Stories feature. Specifically, we want to identify users who had a &#39;view&#39; interaction but then never had a &#39;like&#39; or &#39;share&#39; interaction. How would you identify these users using pandas?",id:"question-3-we-are-concerned-about-user-churn-on-the-stories-feature-specifically-we-want-to-identify-users-who-had-a-view-interaction-but-then-never-had-a-like-or-share-interaction-how-would-you-identify-these-users-using-pandas",level:2},{value:"<strong>Question 4:</strong> We need to analyze user engagement over time. Specifically, we want to calculate the daily average number of interactions per user. How would you do this using pandas?",id:"question-4-we-need-to-analyze-user-engagement-over-time-specifically-we-want-to-calculate-the-daily-average-number-of-interactions-per-user-how-would-you-do-this-using-pandas",level:2},{value:"<strong>Question 5:</strong> We want to understand if there&#39;s a correlation between the number of views a user has and the number of shares they make. How would you calculate this correlation using pandas, and what columns would you use?",id:"question-5-we-want-to-understand-if-theres-a-correlation-between-the-number-of-views-a-user-has-and-the-number-of-shares-they-make-how-would-you-calculate-this-correlation-using-pandas-and-what-columns-would-you-use",level:2},{value:"<strong>Question 6:</strong> We have an external list of &quot;power users&quot; (<code>power_users_df</code>), which contains <code>user_id</code> and a <code>power_user_score</code>. We want to calculate the average number of &#39;like&#39; interactions made by these power users, compared to the average number of &#39;like&#39; interactions made by all other users (non-power users). How would you do this using pandas?",id:"question-6-we-have-an-external-list-of-power-users-power_users_df-which-contains-user_id-and-a-power_user_score-we-want-to-calculate-the-average-number-of-like-interactions-made-by-these-power-users-compared-to-the-average-number-of-like-interactions-made-by-all-other-users-non-power-users-how-would-you-do-this-using-pandas",level:2}];function c(e){const n={code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"context",children:"Context"})}),"\n",(0,i.jsxs)(n.p,{children:['We have a dataset representing user interactions with a new "Stories" feature. Each row in our ',(0,i.jsx)(n.code,{children:"interactions"})," DataFrame corresponds to a single user interaction, and it has the following columns:"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"user_id"})," (str): A unique identifier for each user."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"story_id"})," (str): A unique identifier for each story."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"interaction_type"})," (str): The type of interaction, which can be 'view', 'like', 'share', or 'skip'."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"timestamp"})," (datetime): The time of the interaction."]}),"\n"]}),"\n",(0,i.jsxs)(n.h3,{id:"question-1-how-would-you-use-pandas-to-calculate-the-total-number-of-unique-users-who-viewed-at-least-one-story",children:[(0,i.jsx)(n.strong,{children:"Question 1:"})," How would you use pandas to calculate the total number of unique users who viewed ",(0,i.jsx)(n.em,{children:"at least one"})," story?"]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Benchmark Answer for Question 1:"})}),"\n",(0,i.jsx)(n.p,{children:"A Senior Data Scientist would approach this with a clear understanding of filtering and then identifying unique values."}),"\n",(0,i.jsx)(n.p,{children:"Python"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"import pandas as pd\n\n# Assume 'interactions' DataFrame is already loaded and looks something like this:\ndata = {\n    'user_id': ['A', 'B', 'A', 'C', 'B', 'D', 'A', 'C'],\n    'story_id': ['s1', 's2', 's1', 's3', 's1', 's4', 's5', 's2'],\n    'interaction_type': ['view', 'like', 'share', 'view', 'view', 'skip', 'view', 'like'],\n    'timestamp': pd.to_datetime([\n        '2023-01-01 10:00:00', '2023-01-01 10:05:00', '2023-01-01 10:10:00',\n        '2023-01-01 10:15:00', '2023-01-01 10:20:00', '2023-01-01 10:25:00',\n        '2023-01-01 10:30:00', '2023-01-01 10:35:00'\n    ])\n}\ninteractions = pd.DataFrame(data)\n\n# Filter for 'view' interactions\nview_interactions = interactions[interactions['interaction_type'] == 'view']\n\n# Get the number of unique users from the filtered DataFrame\nnum_unique_viewers = view_interactions['user_id'].nunique()\n\nprint(f\"Total number of unique users who viewed at least one story: {num_unique_viewers}\")\n"})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Explanation of the benchmark answer:"})}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Filtering for 'view' interactions:"})," The first crucial step is to narrow down the DataFrame to only include rows where ",(0,i.jsx)(n.code,{children:"interaction_type"})," is 'view'. This ensures we are only considering the relevant interactions."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsxs)(n.strong,{children:[(0,i.jsx)(n.code,{children:"nunique()"})," for unique count:"]})," After filtering, we simply call ",(0,i.jsx)(n.code,{children:".nunique()"})," on the ",(0,i.jsx)(n.code,{children:"user_id"})," column of the filtered DataFrame. This directly gives us the count of distinct ",(0,i.jsx)(n.code,{children:"user_id"}),"s, which represents the total number of unique users who had at least one 'view' interaction. This is the most direct and idiomatic pandas way to achieve the desired result."]}),"\n"]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"question-2-we-want-to-understand-the-popularity-of-stories-how-would-you-use-pandas-to-find-the-top-5-most-viewed-stories-along-with-the-count-of-views-for-each",children:"Question 2:** We want to understand the popularity of stories. How would you use pandas to find the top 5 most viewed stories, along with the count of views for each?"}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Benchmark Answer for Question 2:"})}),"\n",(0,i.jsx)(n.p,{children:'A Senior Data Scientist would ensure the sorting is in the correct direction to identify the "top" items.'}),"\n",(0,i.jsx)(n.p,{children:"Python"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"import pandas as pd\n\n# Assume 'interactions' DataFrame is already loaded as before\ndata = {\n    'user_id': ['A', 'B', 'A', 'C', 'B', 'D', 'A', 'C', 'E', 'A'],\n    'story_id': ['s1', 's2', 's1', 's3', 's1', 's4', 's5', 's2', 's1', 's3'],\n    'interaction_type': ['view', 'like', 'view', 'view', 'view', 'skip', 'view', 'view', 'view', 'view'],\n    'timestamp': pd.to_datetime([\n        '2023-01-01 10:00:00', '2023-01-01 10:05:00', '2023-01-01 10:10:00',\n        '2023-01-01 10:15:00', '2023-01-01 10:20:00', '2023-01-01 10:25:00',\n        '2023-01-01 10:30:00', '2023-01-01 10:35:00', '2023-01-01 10:40:00',\n        '2023-01-01 10:45:00'\n    ])\n}\ninteractions = pd.DataFrame(data)\n\n# Filter for 'view' interactions (re-defining view_interactions for clarity in this separate example)\nview_interactions = interactions[interactions['interaction_type'] == 'view']\n\n# Calculate the count of views for each story_id and get the top 5\ntop_5_most_viewed_stories = view_interactions['story_id'].value_counts().head(5)\n\nprint(\"Top 5 most viewed stories and their view counts:\")\nprint(top_5_most_viewed_stories)\n"})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Explanation of the benchmark answer:"})}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Filtering for 'view' interactions:"})," As established, we first ensure we are only counting 'view' interactions."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsxs)(n.strong,{children:[(0,i.jsx)(n.code,{children:"value_counts()"}),":"]})," This method is perfect for counting the occurrences of unique values in a Series. By default, ",(0,i.jsx)(n.code,{children:"value_counts()"})," returns the counts in ",(0,i.jsx)(n.em,{children:"descending"}),' order, which is exactly what we need for "top N" analysis.']}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsxs)(n.strong,{children:[(0,i.jsx)(n.code,{children:"head(5)"}),":"]})," This method then conveniently selects the first 5 entries from the resulting Series, which correspond to the 5 most viewed stories."]}),"\n"]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsxs)(n.h2,{id:"question-3-we-are-concerned-about-user-churn-on-the-stories-feature-specifically-we-want-to-identify-users-who-had-a-view-interaction-but-then-never-had-a-like-or-share-interaction-how-would-you-identify-these-users-using-pandas",children:[(0,i.jsx)(n.strong,{children:"Question 3:"})," We are concerned about user \"churn\" on the Stories feature. Specifically, we want to identify users who had a 'view' interaction but then never had a 'like' or 'share' interaction. How would you identify these users using pandas?"]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Benchmark Answer for Question 3:"})}),"\n",(0,i.jsxs)(n.p,{children:["A Senior Data Scientist would likely consider using set operations or direct boolean indexing combined with ",(0,i.jsx)(n.code,{children:"isin()"})," for this type of problem, as it can often be more explicit and performant than a full pivot for specific filter conditions."]}),"\n",(0,i.jsx)(n.p,{children:"Python"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"import pandas as pd\nimport numpy as np\n\n# Assume 'interactions' DataFrame is already loaded\ndata = {\n    'user_id': ['A', 'B', 'A', 'C', 'B', 'D', 'A', 'C', 'E', 'F', 'G'],\n    'story_id': ['s1', 's2', 's1', 's3', 's1', 's4', 's5', 's2', 's6', 's7', 's8'],\n    'interaction_type': ['view', 'like', 'view', 'view', 'view', 'skip', 'view', 'like', 'view', 'view', 'view'],\n    'timestamp': pd.to_datetime([\n        '2023-01-01 10:00:00', '2023-01-01 10:05:00', '2023-01-01 10:10:00',\n        '2023-01-01 10:15:00', '2023-01-01 10:20:00', '2023-01-01 10:25:00',\n        '2023-01-01 10:30:00', '2023-01-01 10:35:00', '2023-01-01 10:40:00',\n        '2023-01-01 10:45:00', '2023-01-01 10:50:00'\n    ])\n}\ninteractions = pd.DataFrame(data)\n\n# 1. Get all unique users who had a 'view' interaction\nviewers = interactions[interactions['interaction_type'] == 'view']['user_id'].unique()\n\n# 2. Get all unique users who had a 'like' interaction\nlikers = interactions[interactions['interaction_type'] == 'like']['user_id'].unique()\n\n# 3. Get all unique users who had a 'share' interaction\nsharers = interactions[interactions['interaction_type'] == 'share']['user_id'].unique()\n\n# Convert to sets for efficient set operations\nviewers_set = set(viewers)\nlikers_set = set(likers)\nsharers_set = set(sharers)\n\n# Find users who viewed but did NOT like AND did NOT share\n# This means they are in the 'viewers_set' AND not in 'likers_set' AND not in 'sharers_set'\nusers_who_viewed_but_no_like_share = viewers_set - (likers_set.union(sharers_set))\n\n# Convert the result back to a list or pandas Series if preferred\nchurn_users = list(users_who_viewed_but_no_like_share)\n\nprint(\"Users who viewed but never liked or shared:\")\nprint(churn_users)\n\n# Alternative using boolean indexing (more concise for simple cases)\n# Get users who had a 'view' interaction\nviewed_users_df = interactions[interactions['interaction_type'] == 'view']\n\n# Get users who had a 'like' or 'share' interaction\nliked_or_shared_users_df = interactions[interactions['interaction_type'].isin(['like', 'share'])]\n\n# Identify users who are in 'viewed_users_df' but NOT in 'liked_or_shared_users_df'\n# We want unique user_ids from viewed_users_df\nunique_viewers = viewed_users_df['user_id'].unique()\nunique_liked_or_shared = liked_or_shared_users_df['user_id'].unique()\n\n# Use pandas isin() for filtering:\nchurn_users_isin = unique_viewers[~np.isin(unique_viewers, unique_liked_or_shared)]\n\nprint(\"\\nUsers who viewed but never liked or shared (using isin()):\")\nprint(churn_users_isin)\n"})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Explanation of the benchmark answer:"})}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Extracting Unique User Sets:"}),' The most robust and often clearest way to handle "did X but not Y" logic is by first getting the unique identifiers for each group of users (',(0,i.jsx)(n.code,{children:"viewers"}),", ",(0,i.jsx)(n.code,{children:"likers"}),", ",(0,i.jsx)(n.code,{children:"sharers"}),")."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Set Operations:"})," Python's built-in ",(0,i.jsx)(n.code,{children:"set"})," type provides highly optimized operations for finding differences and unions.","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"likers_set.union(sharers_set)"})," creates a set of all users who either liked OR shared."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"viewers_set - (likers_set.union(sharers_set))"})," then subtracts this combined set from the ",(0,i.jsx)(n.code,{children:"viewers_set"}),". The result is exactly the users who viewed but are NOT in the group of users who liked or shared."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsxs)(n.strong,{children:["Alternative using ",(0,i.jsx)(n.code,{children:"np.isin()"}),":"]})," For direct pandas manipulation, filtering the unique viewers based on whether they are ",(0,i.jsx)(n.em,{children:"not"})," in the unique set of likers/sharers using ",(0,i.jsx)(n.code,{children:"np.isin()"})," is also very effective and often highly performant."]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"Both of these benchmark approaches are efficient and directly target the problem without needing to create a sparse pivoted DataFrame, which can be memory-intensive for very large datasets with many interaction types."}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsxs)(n.h2,{id:"question-4-we-need-to-analyze-user-engagement-over-time-specifically-we-want-to-calculate-the-daily-average-number-of-interactions-per-user-how-would-you-do-this-using-pandas",children:[(0,i.jsx)(n.strong,{children:"Question 4:"})," We need to analyze user engagement over time. Specifically, we want to calculate the daily average number of interactions per user. How would you do this using pandas?"]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Benchmark Answer for Question 4:"})}),"\n",(0,i.jsx)(n.p,{children:"A Senior Data Scientist would typically break this down into two clear steps: first calculate interactions per user per day, and then average these values."}),"\n",(0,i.jsx)(n.p,{children:"Python"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"import pandas as pd\n\n# Assume 'interactions' DataFrame is already loaded\ndata = {\n    'user_id': ['A', 'B', 'A', 'C', 'B', 'D', 'A', 'C', 'E', 'A', 'A', 'B', 'C'],\n    'story_id': ['s1', 's2', 's1', 's3', 's1', 's4', 's5', 's2', 's6', 's7', 's8', 's9', 's10'],\n    'interaction_type': ['view', 'like', 'view', 'view', 'view', 'skip', 'view', 'like', 'view', 'view', 'share', 'view', 'view'],\n    'timestamp': pd.to_datetime([\n        '2023-01-01 10:00:00', '2023-01-01 10:05:00', '2023-01-01 10:10:00',\n        '2023-01-01 10:15:00', '2023-01-01 10:20:00', '2023-01-01 10:25:00',\n        '2023-01-02 09:00:00', '2023-01-02 09:15:00', '2023-01-02 10:00:00',\n        '2023-01-03 11:00:00', '2023-01-03 11:10:00', '2023-01-03 11:20:00',\n        '2023-01-03 11:30:00'\n    ])\n}\ninteractions = pd.DataFrame(data)\n\n# Step 1: Extract the date from the timestamp\ninteractions['event_date'] = interactions['timestamp'].dt.date\n# Alternatively, for datetime objects normalized to the start of the day:\n# interactions['event_date'] = interactions['timestamp'].dt.normalize()\n\n# Step 2: Calculate the number of interactions per user per day\ninteractions_per_user_per_day = interactions.groupby(['user_id', 'event_date']).size().reset_index(name='interactions_count')\n\n# The resulting DataFrame `interactions_per_user_per_day` now looks like:\n#    user_id  event_date  interactions_count\n# 0        A  2023-01-01                   2\n# 1        A  2023-01-02                   1\n# 2        A  2023-01-03                   2\n# 3        B  2023-01-01                   2\n# 4        B  2023-01-03                   1\n# 5        C  2023-01-01                   1\n# 6        C  2023-01-02                   1\n# 7        C  2023-01-03                   1\n# 8        D  2023-01-01                   1\n# 9        E  2023-01-02                   1\n\n# Step 3: Calculate the overall average of these daily user interaction counts\ndaily_average_interactions_per_user = interactions_per_user_per_day['interactions_count'].mean()\n\nprint(f\"Daily average number of interactions per user: {daily_average_interactions_per_user:.2f}\")\n\n# --- Alternative using `groupby().transform()` or `groupby().apply()` could also work for complex scenarios,\n# --- but the two-step `groupby().size().mean()` is often clearest for this specific question.\n"})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Explanation of the benchmark answer:"})}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsxs)(n.strong,{children:["Extracting ",(0,i.jsx)(n.code,{children:"event_date"}),":"]})," The ",(0,i.jsx)(n.code,{children:"dt.date"})," accessor is used to get just the date component (YYYY-MM-DD) from the ",(0,i.jsx)(n.code,{children:"timestamp"})," column. This is crucial for grouping interactions by day, regardless of the time of day. ",(0,i.jsx)(n.code,{children:"dt.normalize()"})," is another excellent option that keeps the result as a datetime object but with time set to 00:00:00."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Interactions per user per day:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"interactions.groupby(['user_id', 'event_date'])"}),": Groups the DataFrame by both the user and the specific day."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:".size()"}),": This is a very efficient way to count the number of rows within each group. It directly gives you the count of interactions for each ",(0,i.jsx)(n.code,{children:"(user_id, event_date)"})," combination."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:".reset_index(name='interactions_count')"}),": This converts the grouped result (a Series) back into a DataFrame, giving the count column a meaningful name."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Overall Average:"})," Finally, ",(0,i.jsx)(n.code,{children:".mean()"})," is called on the newly created ",(0,i.jsx)(n.code,{children:"interactions_count"})," column of the ",(0,i.jsx)(n.code,{children:"interactions_per_user_per_day"}),' DataFrame. This calculates the average of all the individual daily interaction counts per user, fulfilling the "daily average number of interactions per user" requirement.']}),"\n"]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsxs)(n.h2,{id:"question-5-we-want-to-understand-if-theres-a-correlation-between-the-number-of-views-a-user-has-and-the-number-of-shares-they-make-how-would-you-calculate-this-correlation-using-pandas-and-what-columns-would-you-use",children:[(0,i.jsx)(n.strong,{children:"Question 5:"})," We want to understand if there's a correlation between the number of views a user has and the number of shares they make. How would you calculate this correlation using pandas, and what columns would you use?"]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"import pandas as pd\nimport numpy as np\n\n# Assume 'interactions' DataFrame is already loaded\ndata = {\n    'user_id': ['A', 'B', 'A', 'C', 'B', 'D', 'A', 'C', 'E', 'A', 'B', 'A'],\n    'story_id': ['s1', 's2', 's1', 's3', 's1', 's4', 's5', 's2', 's6', 's7', 's8', 's9'],\n    'interaction_type': ['view', 'like', 'view', 'view', 'view', 'skip', 'share', 'view', 'share', 'view', 'like', 'view'],\n    'timestamp': pd.to_datetime([\n        '2023-01-01 10:00:00', '2023-01-01 10:05:00', '2023-01-01 10:10:00',\n        '2023-01-01 10:15:00', '2023-01-01 10:20:00', '2023-01-01 10:25:00',\n        '2023-01-01 10:30:00', '2023-01-01 10:35:00', '2023-01-01 10:40:00',\n        '2023-01-01 10:45:00', '2023-01-01 10:50:00', '2023-01-01 10:55:00'\n    ])\n}\ninteractions = pd.DataFrame(data)\n\n# Step 1: Calculate total views per user\ntotal_views_per_user = interactions[interactions['interaction_type'] == 'view'] \\\n                                     .groupby('user_id') \\\n                                     .size() \\\n                                     .rename('total_views') # Rename for clarity\n\n# Step 2: Calculate total shares per user\ntotal_shares_per_user = interactions[interactions['interaction_type'] == 'share'] \\\n                                     .groupby('user_id') \\\n                                     .size() \\\n                                     .rename('total_shares') # Rename for clarity\n\n# Step 3: Combine these two Series into a single DataFrame for easier correlation calculation\n# Using pd.merge or pd.concat is common. pd.DataFrame.join is also an option.\n# We'll use a merge for robustness in case some users only have views or shares.\nuser_engagement = pd.merge(\n    total_views_per_user.reset_index(),\n    total_shares_per_user.reset_index(),\n    on='user_id',\n    how='outer' # Use outer to keep all users, even if they have 0 views or shares\n).fillna(0) # Fill NaN with 0 for users who didn't have views or shares for a type\n\n# Step 4: Calculate the correlation between 'total_views' and 'total_shares'\n# Using .corr() on the DataFrame directly between the two columns\ncorrelation_views_shares = user_engagement['total_views'].corr(user_engagement['total_shares'])\n\nprint(f\"Correlation between total views and total shares per user: {correlation_views_shares:.4f}\")\n\n# The columns used for correlation are 'total_views' and 'total_shares' from the\n# 'user_engagement' DataFrame.\n\n# --- Alternative: Correlation of Daily Counts per User-Day ---\n# If the question implied correlating daily patterns, the approach would be:\n\n# interactions['event_date'] = interactions['timestamp'].dt.date\n#\n# daily_counts = interactions.groupby(['user_id', 'event_date', 'interaction_type']).size().unstack(fill_value=0)\n#\n# # Ensure 'view' and 'share' columns exist, fill with 0 if not\n# if 'view' not in daily_counts.columns:\n#     daily_counts['view'] = 0\n# if 'share' not in daily_counts.columns:\n#     daily_counts['share'] = 0\n#\n# # Now, correlate the 'view' and 'share' columns from this daily_counts DataFrame\n# daily_correlation = daily_counts['view'].corr(daily_counts['share'])\n# print(f\"\\nCorrelation between daily views and daily shares (across all user-days): {daily_correlation:.4f}\")\n"})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Explanation of the benchmark answer:"})}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Aggregate Total Views and Shares per User:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["We first filter the ",(0,i.jsx)(n.code,{children:"interactions"})," DataFrame for ",(0,i.jsx)(n.code,{children:"interaction_type == 'view'"})," and ",(0,i.jsx)(n.code,{children:"interaction_type == 'share'"})," separately."]}),"\n",(0,i.jsxs)(n.li,{children:["Then, ",(0,i.jsx)(n.code,{children:"groupby('user_id').size()"})," efficiently counts the number of interactions for each user for that specific type."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:".rename()"})," gives these Series meaningful names (",(0,i.jsx)(n.code,{children:"total_views"}),", ",(0,i.jsx)(n.code,{children:"total_shares"}),")."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Combine Aggregated Data:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"pd.merge()"})," is used to combine these two Series (after ",(0,i.jsx)(n.code,{children:"reset_index()"})," to make ",(0,i.jsx)(n.code,{children:"user_id"})," a column again) into a single DataFrame (",(0,i.jsx)(n.code,{children:"user_engagement"}),")."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"how='outer'"})," ensures that users who only viewed but never shared, or vice-versa, are included."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:".fillna(0)"})," is crucial because if a user had no views or no shares, the corresponding column in the merged DataFrame would have ",(0,i.jsx)(n.code,{children:"NaN"}),". For correlation purposes, ",(0,i.jsx)(n.code,{children:"NaN"}),' values are typically treated as missing, and we want to correctly represent "zero interactions."']}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Calculate Correlation:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Finally, the ",(0,i.jsx)(n.code,{children:".corr()"})," method is called between the ",(0,i.jsx)(n.code,{children:"total_views"})," and ",(0,i.jsx)(n.code,{children:"total_shares"})," columns of the ",(0,i.jsx)(n.code,{children:"user_engagement"})," DataFrame. This directly computes the Pearson correlation coefficient, which is the standard measure for this type of question."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"The alternative approach for daily correlation is also shown, demonstrating how you would approach it if the daily granularity were explicitly required."}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsxs)(n.h2,{id:"question-6-we-have-an-external-list-of-power-users-power_users_df-which-contains-user_id-and-a-power_user_score-we-want-to-calculate-the-average-number-of-like-interactions-made-by-these-power-users-compared-to-the-average-number-of-like-interactions-made-by-all-other-users-non-power-users-how-would-you-do-this-using-pandas",children:[(0,i.jsx)(n.strong,{children:"Question 6:"}),' We have an external list of "power users" (',(0,i.jsx)(n.code,{children:"power_users_df"}),"), which contains ",(0,i.jsx)(n.code,{children:"user_id"})," and a ",(0,i.jsx)(n.code,{children:"power_user_score"}),". We want to calculate the average number of 'like' interactions made by these power users, compared to the average number of 'like' interactions made by all other users (non-power users). How would you do this using pandas?"]}),"\n",(0,i.jsxs)(n.p,{children:["Assume ",(0,i.jsx)(n.code,{children:"power_users_df"})," looks like this:"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"  user_id  power_user_score\n0      A                95\n1      B                88\n2      F                92\n"})}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Benchmark Answer for Question 6:"})}),"\n",(0,i.jsx)(n.p,{children:"A Senior Data Scientist would approach this by:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"First, calculate the total 'like' interactions for every user."}),"\n",(0,i.jsxs)(n.li,{children:["Then, merge this data with the ",(0,i.jsx)(n.code,{children:"power_users_df"})," to categorize users as 'power' or 'non-power'."]}),"\n",(0,i.jsx)(n.li,{children:"Finally, group by this new category and calculate the average 'like' interactions for each group."}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"Python"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"import pandas as pd\nimport numpy as np\n\n# Assume 'interactions' DataFrame is already loaded\ndata = {\n    'user_id': ['A', 'B', 'A', 'C', 'B', 'D', 'A', 'C', 'E', 'F', 'G', 'A', 'H', 'I'],\n    'story_id': ['s1', 's2', 's1', 's3', 's1', 's4', 's5', 's2', 's6', 's7', 's8', 's9', 's10', 's11'],\n    'interaction_type': ['view', 'like', 'view', 'view', 'like', 'skip', 'share', 'like', 'view', 'like', 'view', 'view', 'like', 'view'],\n    'timestamp': pd.to_datetime([\n        '2023-01-01 10:00:00', '2023-01-01 10:05:00', '2023-01-01 10:10:00',\n        '2023-01-01 10:15:00', '2023-01-01 10:20:00', '2023-01-01 10:25:00',\n        '2023-01-01 10:30:00', '2023-01-01 10:35:00', '2023-01-01 10:40:00',\n        '2023-01-01 10:45:00', '2023-01-01 10:50:00', '2023-01-01 10:55:00',\n        '2023-01-02 09:00:00', '2023-01-02 09:05:00'\n    ])\n}\ninteractions = pd.DataFrame(data)\n\n# Power users DataFrame as provided in the question\npower_users_data = {\n    'user_id': ['A', 'B', 'F'],\n    'power_user_score': [95, 88, 92]\n}\npower_users_df = pd.DataFrame(power_users_data)\n\n\n# Step 1: Calculate the total number of 'like' interactions for each user\n# We use .size() to count interactions per group, and .rename() for clarity.\nlikes_per_user = interactions[interactions['interaction_type'] == 'like'] \\\n                                .groupby('user_id') \\\n                                .size() \\\n                                .rename('total_likes') \\\n                                .reset_index() # Convert Series to DataFrame for merging\n\n# Step 2: Merge with power_users_df to identify power users and non-power users\n# Use a left merge from 'likes_per_user' to keep all users who liked,\n# and then add a flag for power users.\nuser_likes_with_power_status = pd.merge(\n    likes_per_user,\n    power_users_df[['user_id']], # Only need user_id from power_users_df for the join\n    on='user_id',\n    how='left',\n    indicator='_merge' # Add a column indicating merge source (left_only, right_only, both)\n)\n\n# Create a 'user_type' column based on whether the user_id was found in power_users_df\nuser_likes_with_power_status['user_type'] = np.where(\n    user_likes_with_power_status['_merge'] == 'both',\n    'power_user',\n    'non_power_user'\n)\n\n# Clean up the merge indicator column\nuser_likes_with_power_status = user_likes_with_power_status.drop(columns=['_merge'])\n\n# Step 3: Calculate the average number of 'like' interactions for each user group\naverage_likes_by_group = user_likes_with_power_status.groupby('user_type')['total_likes'].mean()\n\nprint(\"Average number of 'like' interactions by user type:\")\nprint(average_likes_by_group)\n\n# To include users who had 0 likes but are part of the population:\n# If you need to consider *all* users from the original interactions DataFrame,\n# even those with 0 likes, the approach would be slightly different:\n\n# Get all unique user IDs from the main interactions dataset\nall_users = pd.DataFrame(interactions['user_id'].unique(), columns=['user_id'])\n\n# Left merge all users with their like counts, filling NaN for those with 0 likes\nall_users_likes = pd.merge(\n    all_users,\n    likes_per_user,\n    on='user_id',\n    how='left'\n).fillna({'total_likes': 0}) # Fill NaN total_likes with 0\n\n# Now merge with power_users_df to classify all users\nall_users_classified = pd.merge(\n    all_users_likes,\n    power_users_df[['user_id']],\n    on='user_id',\n    how='left',\n    indicator='_merge'\n)\n\nall_users_classified['user_type'] = np.where(\n    all_users_classified['_merge'] == 'both',\n    'power_user',\n    'non_power_user'\n)\nall_users_classified = all_users_classified.drop(columns=['_merge'])\n\naverage_likes_by_group_all_users = all_users_classified.groupby('user_type')['total_likes'].mean()\nprint(\"\\nAverage number of 'like' interactions by user type (including users with 0 likes):\")\nprint(average_likes_by_group_all_users)\n\n\n# If you were to perform a t-test (beyond pandas, using scipy.stats):\n# from scipy import stats\n#\n# power_user_likes = all_users_classified[all_users_classified['user_type'] == 'power_user']['total_likes']\n# non_power_user_likes = all_users_classified[all_users_classified['user_type'] == 'non_power_user']['total_likes']\n#\n# # Perform independent t-test (assuming equal variances for now)\n# t_stat, p_value = stats.ttest_ind(power_user_likes, non_power_user_likes, equal_var=True)\n# print(f\"\\nResults of Independent t-test:\")\n# print(f\"T-statistic: {t_stat:.4f}\")\n# print(f\"P-value: {p_value:.4f}\")\n"})})]})}function u(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(c,{...e})}):c(e)}}}]);