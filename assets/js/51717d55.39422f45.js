"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[14814],{28453:(e,n,t)=>{t.d(n,{R:()=>i,x:()=>a});var s=t(96540);const r={},o=s.createContext(r);function i(e){const n=s.useContext(o);return s.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:i(e.components),s.createElement(o.Provider,{value:n},e.children)}},62547:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>h,frontMatter:()=>i,metadata:()=>s,toc:()=>d});const s=JSON.parse('{"id":"slip-box/reference-notes/How does GPT work","title":"How does GPT work","description":"\u2317 Metadata","source":"@site/docs/slip-box/reference-notes/How does GPT work.md","sourceDirName":"slip-box/reference-notes","slug":"/slip-box/reference-notes/How does GPT work","permalink":"/docs/slip-box/reference-notes/How does GPT work","draft":false,"unlisted":false,"editUrl":"https://github.com/paulliwali/paulliwali.github.io/tree/main/docs/docs/slip-box/reference-notes/How does GPT work.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Hikes Bucket List","permalink":"/docs/slip-box/reference-notes/Hikes Bucket List"},"next":{"title":"\ud83d\udcf0 Summary (use your own words)","permalink":"/docs/slip-box/reference-notes/How not to run an AB test"}}');var r=t(74848),o=t(28453);const i={},a=void 0,l={},d=[];function c(e){const n={a:"a",blockquote:"blockquote",code:"code",em:"em",li:"li",p:"p",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.p,{children:"\u2317 Metadata"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["Source:","\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://confusedbit.dev/posts/how_does_gpt_work/",children:"https://confusedbit.dev/posts/how_does_gpt_work/"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://jalammar.github.io/illustrated-transformer/",children:"https://jalammar.github.io/illustrated-transformer/"})}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.li,{children:"Author:"}),"\n",(0,r.jsx)(n.li,{children:"Tags: #programming #machine-learning"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:'\ud83d\udcf0 Summary (use your own words)\nStems from Google\'s seminal paper "Attention is all you need" which outlined the foundation of a transformer. This transformer structure is the key to drastically improve the natural language processing.'}),"\n",(0,r.jsx)(n.p,{children:"\u270d\ufe0f Notes"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Generalized pre-trained transformer"}),"\n",(0,r.jsxs)(n.li,{children:["The baseline model for natural language is a Markov chain","\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Take a reference text and learn the probabilities of word sequences"}),"\n"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.blockquote,{children:["\n",(0,r.jsx)(n.p,{children:"The cat eats the rat"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["This learns that after ",(0,r.jsx)(n.code,{children:"the"})," there is a 50/50 chance that the next word is cat or rat"]}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsxs)(n.strong,{children:["This model fails to understand ",(0,r.jsx)(n.em,{children:"context"})]})}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["To improve this, the Transformer architecture gives ",(0,r.jsx)(n.em,{children:"attention"})," to words","\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"The Transformer is comprised a stack of encoders and a stack of decoders"}),"\n",(0,r.jsx)(n.li,{children:"Each encoder is comprised of self-attention layer and a feed forward neural network"}),"\n",(0,r.jsxs)(n.li,{children:["Each decoder is comprised of a self-attention layer and a feed forward neural network but ",(0,r.jsx)(n.em,{children:"with a encoder-decoder attention layer"})," in between that focuses on relevant parts of the input sequence"]}),"\n",(0,r.jsxs)(n.li,{children:["To pass the inputs around these layers, they have to be embedded into numbers through an embedding algorithm","\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Happens once in the bottom most encoder"}),"\n",(0,r.jsx)(n.li,{children:"All encoders will receive a 512 sized vector after the embedding"}),"\n",(0,r.jsx)(n.li,{children:"The words are passed on independent paths, but the self-attention layer is what gives the better encoding to each word as it tries to understand the context"}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.li,{children:'Self-attention layer calculates additional vectors: queries, keys, values to help calculate the "attention" vector that gets passed into the feed forward neural network'}),"\n",(0,r.jsx)(n.li,{children:"Multi-headed attention expands the model's ability to focus a several parts"}),"\n"]}),"\n"]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(c,{...e})}):c(e)}}}]);