"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[50292],{3905:(e,t,r)=>{r.d(t,{Zo:()=>c,kt:()=>f});var a=r(67294);function n(e,t,r){return t in e?Object.defineProperty(e,t,{value:r,enumerable:!0,configurable:!0,writable:!0}):e[t]=r,e}function o(e,t){var r=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),r.push.apply(r,a)}return r}function l(e){for(var t=1;t<arguments.length;t++){var r=null!=arguments[t]?arguments[t]:{};t%2?o(Object(r),!0).forEach((function(t){n(e,t,r[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(r)):o(Object(r)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(r,t))}))}return e}function i(e,t){if(null==e)return{};var r,a,n=function(e,t){if(null==e)return{};var r,a,n={},o=Object.keys(e);for(a=0;a<o.length;a++)r=o[a],t.indexOf(r)>=0||(n[r]=e[r]);return n}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(a=0;a<o.length;a++)r=o[a],t.indexOf(r)>=0||Object.prototype.propertyIsEnumerable.call(e,r)&&(n[r]=e[r])}return n}var s=a.createContext({}),p=function(e){var t=a.useContext(s),r=t;return e&&(r="function"==typeof e?e(t):l(l({},t),e)),r},c=function(e){var t=p(e.components);return a.createElement(s.Provider,{value:t},e.children)},u="mdxType",d={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},m=a.forwardRef((function(e,t){var r=e.components,n=e.mdxType,o=e.originalType,s=e.parentName,c=i(e,["components","mdxType","originalType","parentName"]),u=p(r),m=n,f=u["".concat(s,".").concat(m)]||u[m]||d[m]||o;return r?a.createElement(f,l(l({ref:t},c),{},{components:r})):a.createElement(f,l({ref:t},c))}));function f(e,t){var r=arguments,n=t&&t.mdxType;if("string"==typeof e||n){var o=r.length,l=new Array(o);l[0]=m;var i={};for(var s in t)hasOwnProperty.call(t,s)&&(i[s]=t[s]);i.originalType=e,i[u]="string"==typeof e?e:n,l[1]=i;for(var p=2;p<o;p++)l[p]=r[p];return a.createElement.apply(null,l)}return a.createElement.apply(null,r)}m.displayName="MDXCreateElement"},20120:(e,t,r)=>{r.r(t),r.d(t,{assets:()=>s,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>p});var a=r(87462),n=(r(67294),r(3905));const o={},l=void 0,i={unversionedId:"slip-box/reference-notes/How does GPT work",id:"slip-box/reference-notes/How does GPT work",title:"How does GPT work",description:"\u2317 Metadata",source:"@site/docs/slip-box/reference-notes/How does GPT work.md",sourceDirName:"slip-box/reference-notes",slug:"/slip-box/reference-notes/How does GPT work",permalink:"/docs/slip-box/reference-notes/How does GPT work",draft:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/slip-box/reference-notes/How does GPT work.md",tags:[],version:"current",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Hike the West Coast Trail",permalink:"/docs/slip-box/reference-notes/Hike the West Coast Trail"},next:{title:"How to Become a Data Engineer",permalink:"/docs/slip-box/reference-notes/How to Become a Data Engineer"}},s={},p=[],c={toc:p};function u(e){let{components:t,...r}=e;return(0,n.kt)("wrapper",(0,a.Z)({},c,r,{components:t,mdxType:"MDXLayout"}),(0,n.kt)("p",null,"\u2317 Metadata"),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},"Source: ",(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://confusedbit.dev/posts/how_does_gpt_work/"},"https://confusedbit.dev/posts/how_does_gpt_work/")),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://jalammar.github.io/illustrated-transformer/"},"https://jalammar.github.io/illustrated-transformer/")))),(0,n.kt)("li",{parentName:"ul"},"Author: "),(0,n.kt)("li",{parentName:"ul"},"Tags: #programming #machine-learning ")),(0,n.kt)("p",null,'\ud83d\udcf0 Summary (use your own words)\nStems from Google\'s seminal paper "Attention is all you need" which outlined the foundation of a transformer. This transformer structure is the key to drastically improve the natural language processing.'),(0,n.kt)("p",null,"\u270d\ufe0f Notes"),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},"Generalized pre-trained transformer"),(0,n.kt)("li",{parentName:"ul"},"The baseline model for natural language is a Markov chain",(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},"Take a reference text and learn the probabilities of word sequences")),(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("blockquote",{parentName:"li"},(0,n.kt)("p",{parentName:"blockquote"},"The cat eats the rat")))),(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},"This learns that after ",(0,n.kt)("inlineCode",{parentName:"li"},"the")," there is a 50/50 chance that the next word is cat or rat"),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("strong",{parentName:"li"},"This model fails to understand ",(0,n.kt)("em",{parentName:"strong"},"context"))))),(0,n.kt)("li",{parentName:"ul"},"To improve this, the Transformer architecture gives ",(0,n.kt)("em",{parentName:"li"},"attention")," to words",(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},"The Transformer is comprised a stack of encoders and a stack of decoders "),(0,n.kt)("li",{parentName:"ul"},"Each encoder is comprised of self-attention layer and a feed forward neural network"),(0,n.kt)("li",{parentName:"ul"},"Each decoder is comprised of a self-attention layer and a feed forward neural network but ",(0,n.kt)("em",{parentName:"li"},"with a encoder-decoder attention layer")," in between that focuses on relevant parts of the input sequence"),(0,n.kt)("li",{parentName:"ul"},"To pass the inputs around these layers, they have to be embedded into numbers through an embedding algorithm",(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},"Happens once in the bottom most encoder"),(0,n.kt)("li",{parentName:"ul"},"All encoders will receive a 512 sized vector after the embedding"),(0,n.kt)("li",{parentName:"ul"},"The words are passed on independent paths, but the self-attention layer is what gives the better encoding to each word as it tries to understand the context "))),(0,n.kt)("li",{parentName:"ul"},'Self-attention layer calculates additional vectors: queries, keys, values to help calculate the "attention" vector that gets passed into the feed forward neural network'),(0,n.kt)("li",{parentName:"ul"},"Multi-headed attention expands the model's ability to focus a several parts")))))}u.isMDXComponent=!0}}]);