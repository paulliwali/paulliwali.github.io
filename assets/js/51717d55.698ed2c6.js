"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[50292],{81655:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>i,default:()=>h,frontMatter:()=>o,metadata:()=>a,toc:()=>d});var s=t(85893),r=t(11151);const o={},i=void 0,a={id:"slip-box/reference-notes/How does GPT work",title:"How does GPT work",description:"\u2317 Metadata",source:"@site/docs/slip-box/reference-notes/How does GPT work.md",sourceDirName:"slip-box/reference-notes",slug:"/slip-box/reference-notes/How does GPT work",permalink:"/docs/slip-box/reference-notes/How does GPT work",draft:!1,unlisted:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/slip-box/reference-notes/How does GPT work.md",tags:[],version:"current",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Hike the West Coast Trail",permalink:"/docs/slip-box/reference-notes/Hike the West Coast Trail"},next:{title:"How to Become a Data Engineer",permalink:"/docs/slip-box/reference-notes/How to Become a Data Engineer"}},l={},d=[];function c(e){const n={a:"a",blockquote:"blockquote",code:"code",em:"em",li:"li",p:"p",strong:"strong",ul:"ul",...(0,r.a)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.p,{children:"\u2317 Metadata"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["Source:","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://confusedbit.dev/posts/how_does_gpt_work/",children:"https://confusedbit.dev/posts/how_does_gpt_work/"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://jalammar.github.io/illustrated-transformer/",children:"https://jalammar.github.io/illustrated-transformer/"})}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.li,{children:"Author:"}),"\n",(0,s.jsx)(n.li,{children:"Tags: #programming #machine-learning"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:'\ud83d\udcf0 Summary (use your own words)\nStems from Google\'s seminal paper "Attention is all you need" which outlined the foundation of a transformer. This transformer structure is the key to drastically improve the natural language processing.'}),"\n",(0,s.jsx)(n.p,{children:"\u270d\ufe0f Notes"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Generalized pre-trained transformer"}),"\n",(0,s.jsxs)(n.li,{children:["The baseline model for natural language is a Markov chain","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Take a reference text and learn the probabilities of word sequences"}),"\n"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.blockquote,{children:["\n",(0,s.jsx)(n.p,{children:"The cat eats the rat"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["This learns that after ",(0,s.jsx)(n.code,{children:"the"})," there is a 50/50 chance that the next word is cat or rat"]}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsxs)(n.strong,{children:["This model fails to understand ",(0,s.jsx)(n.em,{children:"context"})]})}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["To improve this, the Transformer architecture gives ",(0,s.jsx)(n.em,{children:"attention"})," to words","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"The Transformer is comprised a stack of encoders and a stack of decoders"}),"\n",(0,s.jsx)(n.li,{children:"Each encoder is comprised of self-attention layer and a feed forward neural network"}),"\n",(0,s.jsxs)(n.li,{children:["Each decoder is comprised of a self-attention layer and a feed forward neural network but ",(0,s.jsx)(n.em,{children:"with a encoder-decoder attention layer"})," in between that focuses on relevant parts of the input sequence"]}),"\n",(0,s.jsxs)(n.li,{children:["To pass the inputs around these layers, they have to be embedded into numbers through an embedding algorithm","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Happens once in the bottom most encoder"}),"\n",(0,s.jsx)(n.li,{children:"All encoders will receive a 512 sized vector after the embedding"}),"\n",(0,s.jsx)(n.li,{children:"The words are passed on independent paths, but the self-attention layer is what gives the better encoding to each word as it tries to understand the context"}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.li,{children:'Self-attention layer calculates additional vectors: queries, keys, values to help calculate the "attention" vector that gets passed into the feed forward neural network'}),"\n",(0,s.jsx)(n.li,{children:"Multi-headed attention expands the model's ability to focus a several parts"}),"\n"]}),"\n"]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,r.a)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(c,{...e})}):c(e)}},11151:(e,n,t)=>{t.d(n,{Z:()=>a,a:()=>i});var s=t(67294);const r={},o=s.createContext(r);function i(e){const n=s.useContext(o);return s.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:i(e.components),s.createElement(o.Provider,{value:n},e.children)}}}]);