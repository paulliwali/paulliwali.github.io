"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[98198],{18581:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>c,contentTitle:()=>l,default:()=>h,frontMatter:()=>t,metadata:()=>a,toc:()=>o});var s=i(85893),r=i(11151);const t={tags:["cheatsheet","data-science"]},l=void 0,a={id:"slip-box/reference-notes/Data Science Cheat Sheet",title:"Data Science Cheat Sheet",description:"Inspired by this https://python-data-science.readthedocs.io/en/latest/_images/architecture.png",source:"@site/docs/slip-box/reference-notes/Data Science Cheat Sheet.md",sourceDirName:"slip-box/reference-notes",slug:"/slip-box/reference-notes/Data Science Cheat Sheet",permalink:"/docs/slip-box/reference-notes/Data Science Cheat Sheet",draft:!1,unlisted:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/slip-box/reference-notes/Data Science Cheat Sheet.md",tags:[{label:"cheatsheet",permalink:"/docs/tags/cheatsheet"},{label:"data-science",permalink:"/docs/tags/data-science"}],version:"current",frontMatter:{tags:["cheatsheet","data-science"]},sidebar:"tutorialSidebar",previous:{title:"Daily Stoic",permalink:"/docs/slip-box/reference-notes/Daily Stoic"},next:{title:"Data Science Learning",permalink:"/docs/slip-box/reference-notes/Data Science Learning"}},c={},o=[];function d(e){const n={a:"a",blockquote:"blockquote",li:"li",ol:"ol",p:"p",ul:"ul",...(0,r.a)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsxs)(n.blockquote,{children:["\n",(0,s.jsxs)(n.p,{children:["Inspired by this ",(0,s.jsx)(n.a,{href:"https://python-data-science.readthedocs.io/en/latest/_images/architecture.png",children:"https://python-data-science.readthedocs.io/en/latest/_images/architecture.png"})]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"MLOps flow: DESIGN + MODEL DEVELOPMENT + OPERATIONS"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Design: requirements engineering, use case prioritization, data availability check"}),"\n",(0,s.jsx)(n.li,{children:"Model development: data engineering, model engineering, testing and validation"}),"\n",(0,s.jsx)(n.li,{children:"Operations: model deployment, CI/CD pipelines, monitoring and alerting"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"Model Design"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["Exploratory Data Analysis","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"[[Distributions]]"}),"\n",(0,s.jsxs)(n.li,{children:["Boxplots - gives descriptions of the data with respect to outliers and median","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Gives the p25 to p75 range which is the IQR"}),"\n",(0,s.jsx)(n.li,{children:"Also gives the min and max which is defined as Q1 - 1.5 * IQR or Q3 + 1.5 * IQR"}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.li,{children:"[[Correlations]]"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["Data Preparation","\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["Feature Preprocessing","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["Fill missing values or remove the column entirely if too many values are missing","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Impute with mean or median, interpolation with linear or other methods"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["Remove outliers to ensure robustness of sensitive models","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"As identified in box plots"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["Encode features from string into integer","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"sine/cosine transformation to maintain cyclic relationships"}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.li,{children:"[[Split training and testing]] to avoid overfitting of the model the training data and avoid data leakage"}),"\n",(0,s.jsx)(n.li,{children:"[[Feature Scaling Techniques]] to bring features into the same space for model convergence"}),"\n",(0,s.jsx)(n.li,{children:"[[Feature Engineering]] to create powerful features that is more informative for the model"}),"\n",(0,s.jsx)(n.li,{children:"[[Class balancing]] to help models predict rare occurrences"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["Model Selection","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["[[Supervised Learning]]","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["Regression","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"[[Linear regression model]]"}),"\n",(0,s.jsx)(n.li,{children:"Polynomial regression model"}),"\n",(0,s.jsx)(n.li,{children:"[[Quantile Regression]]"}),"\n",(0,s.jsx)(n.li,{children:"[[Tree Regressors]]"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["Classification","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"[[KNN]]"}),"\n",(0,s.jsx)(n.li,{children:"[[Naive Bayes]]"}),"\n",(0,s.jsx)(n.li,{children:"[[Support Vector Machines]]"}),"\n",(0,s.jsx)(n.li,{children:"[[Logistic Regression]]"}),"\n",(0,s.jsx)(n.li,{children:"[[Decision Trees]]"}),"\n",(0,s.jsx)(n.li,{children:"[[Tree Ensembles]]"}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["Unsupervised Learning","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"[[Kernel Regression]]"}),"\n",(0,s.jsx)(n.li,{children:"[[k-means]]"}),"\n",(0,s.jsx)(n.li,{children:"[[Gaussian Mixture Models]]"}),"\n",(0,s.jsx)(n.li,{children:"[[DBSCAN]]"}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.li,{children:"Semi-supervised Learning"}),"\n",(0,s.jsx)(n.li,{children:"Reinforcement Learning"}),"\n",(0,s.jsx)(n.li,{children:"[[Recommendation Systems]]"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["Model Fit","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["Loss Functions","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"L1 loss function (sum of the absolute errors)"}),"\n",(0,s.jsx)(n.li,{children:"L2 loss function (sum of the squared errors)"}),"\n",(0,s.jsx)(n.li,{children:"Pinball loss function"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["Hyperparameter tuning result in change in model performance and combined with cross-validation techniques we can find the best set of hyperparameters","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Grid Search"}),"\n",(0,s.jsx)(n.li,{children:"Random Search"}),"\n",(0,s.jsx)(n.li,{children:"Bayesian Optimization"}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["Model Evaluation","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Commonly used [[Error metrics]] to evaluate regression type models"}),"\n",(0,s.jsxs)(n.li,{children:["Bias-variance tradeoff","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"The best predictive model is one that has good generalization ability which is able to predict accurately to new and previously unseen data"}),"\n",(0,s.jsx)(n.li,{children:"high bias can lead to okay performance but too general -> under-fit"}),"\n",(0,s.jsx)(n.li,{children:"high variance can lead to low errors with existing data but not necessarily with new data -> overfit"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["[[Regularization Techniques]]","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"L1 regularization (LASSO) which reduces the coefficient values"}),"\n",(0,s.jsx)(n.li,{children:"L2 regularization (RIDGE) which penalizes higher powers"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["Model fit vs complexity","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"AIC"}),"\n",(0,s.jsx)(n.li,{children:"BIC"}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["Model Score","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Score on unseen data as the true evaluation of the model"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["Serialization","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Pickling of the entire process of fitted data preparation and fitted model parameters"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"Statical hypothesis testing"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"null hypothesis"}),"\n",(0,s.jsx)(n.li,{children:"alpha"}),"\n",(0,s.jsx)(n.li,{children:"beta"}),"\n",(0,s.jsx)(n.li,{children:"p_value"}),"\n",(0,s.jsx)(n.li,{children:"Criticism - should rely less on p-value and more on confidence intervals for effect sizes for importance, prediction intervals for confidence, replication and extension for robustness"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"Bayesian Approach"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["Credible interval","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Bayesian approach is that given the observed data about a parameter, we can assume a prior distribution for it and as we observe more data fit a posteri distribution where it is 95% of the prior"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"Frequentist Approach"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["Confidence interval","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"There is a single value for the parameter and we are getting observed data about the parameter which gives us variance. We can provide the mean estimate of the parameter and a range in which if we repeat this experiment many times we are confident that 95% of the time the value falls within this interval"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["Prediction interval","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Associated with the variance of future values and gives the range which the forecasted value can fall"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"Causal Inference"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"[[AB Testing]]"}),"\n",(0,s.jsxs)(n.li,{children:["Difference in difference","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Fit a regression to the two groups to the observed metric and the coefficient estimated is the difference"}),"\n"]}),"\n"]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,r.a)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},11151:(e,n,i)=>{i.d(n,{Z:()=>a,a:()=>l});var s=i(67294);const r={},t=s.createContext(r);function l(e){const n=s.useContext(t);return s.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:l(e.components),s.createElement(t.Provider,{value:n},e.children)}}}]);