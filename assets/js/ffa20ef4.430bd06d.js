"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[12436],{22544:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>o,contentTitle:()=>a,default:()=>h,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var i=s(85893),t=s(11151);const r={},a="Week 1: Introduction to Machine Learning",l={id:"slip-box/reference-notes/Supervised Machine Learning - Regression and Classification",title:"Week 1: Introduction to Machine Learning",description:"Supervised Learning",source:"@site/docs/slip-box/reference-notes/Supervised Machine Learning - Regression and Classification.md",sourceDirName:"slip-box/reference-notes",slug:"/slip-box/reference-notes/Supervised Machine Learning - Regression and Classification",permalink:"/docs/slip-box/reference-notes/Supervised Machine Learning - Regression and Classification",draft:!1,unlisted:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/slip-box/reference-notes/Supervised Machine Learning - Regression and Classification.md",tags:[],version:"current",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Studying Studies",permalink:"/docs/slip-box/reference-notes/Studying Studies"},next:{title:"Support Vector Machines",permalink:"/docs/slip-box/reference-notes/Support Vector Machines"}},o={},c=[{value:"Supervised Learning",id:"supervised-learning",level:2},{value:"Unsupervised Learning",id:"unsupervised-learning",level:2},{value:"[[Linear Regression Model]]",id:"linear-regression-model",level:2},{value:"Multiple Linear Regression",id:"multiple-linear-regression",level:2},{value:"Gradient descent in practice",id:"gradient-descent-in-practice",level:2}];function d(e){const n={annotation:"annotation",code:"code",em:"em",h1:"h1",h2:"h2",li:"li",math:"math",mi:"mi",mo:"mo",mrow:"mrow",p:"p",semantics:"semantics",span:"span",ul:"ul",...(0,t.a)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.h1,{id:"week-1-introduction-to-machine-learning",children:"Week 1: Introduction to Machine Learning"}),"\n",(0,i.jsx)(n.h2,{id:"supervised-learning",children:"Supervised Learning"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["The most common types of algorithms being used today","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"learns from 'input' to 'output label'"}),"\n",(0,i.jsx)(n.li,{children:"use cases: spam filtering, speech recognition, machine translation, online advertising, self driving"}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:['Gives the algorithm datasets to learn from labelled "correct answers"',"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.em,{children:"Regression modeling"})," is to predict a range of numbers"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.em,{children:"Classification modeling"})," is to predict a limited set of class/category"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"unsupervised-learning",children:"Unsupervised Learning"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Learns from unlabelled output data to find patterns or interesting properties","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"learns from 'input' only as there are no 'output label'"}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.li,{children:"Anomaly detection"}),"\n",(0,i.jsx)(n.li,{children:"Clustering"}),"\n",(0,i.jsx)(n.li,{children:"Dimensionality reduction"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"linear-regression-model",children:"[[Linear Regression Model]]"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Fitting a ",(0,i.jsx)(n.em,{children:"straight line"})," to a dataset","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"produce a function that produces a y-hat (estimated y)"}),"\n",(0,i.jsxs)(n.li,{children:["formulated as a ",(0,i.jsx)(n.code,{children:"wx+b"})," since it is a linear function"]}),"\n",(0,i.jsx)(n.li,{children:"to train/adjust the unknown parameters/coefficients/weights of the function we need to construct a cost function"}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Cost function is a way to quantitatively evaluate the goodness of fit of the function","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsxs)(n.span,{className:"katex",children:[(0,i.jsx)(n.span,{className:"katex-mathml",children:(0,i.jsx)(n.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,i.jsxs)(n.semantics,{children:[(0,i.jsxs)(n.mrow,{children:[(0,i.jsx)(n.mi,{children:"J"}),(0,i.jsx)(n.mo,{stretchy:"false",children:"("}),(0,i.jsx)(n.mi,{children:"w"}),(0,i.jsx)(n.mo,{separator:"true",children:","}),(0,i.jsx)(n.mi,{children:"b"}),(0,i.jsx)(n.mo,{stretchy:"false",children:")"}),(0,i.jsx)(n.mo,{children:"="}),(0,i.jsx)(n.mi,{children:"r"}),(0,i.jsx)(n.mi,{children:"m"}),(0,i.jsx)(n.mi,{children:"s"}),(0,i.jsx)(n.mi,{children:"e"}),(0,i.jsx)(n.mi,{mathvariant:"normal",children:"/"}),(0,i.jsx)(n.mi,{children:"m"}),(0,i.jsx)(n.mi,{children:"s"}),(0,i.jsx)(n.mi,{children:"e"})]}),(0,i.jsx)(n.annotation,{encoding:"application/x-tex",children:"J(w, b) = rmse/mse"})]})})}),(0,i.jsxs)(n.span,{className:"katex-html","aria-hidden":"true",children:[(0,i.jsxs)(n.span,{className:"base",children:[(0,i.jsx)(n.span,{className:"strut",style:{height:"1em",verticalAlign:"-0.25em"}}),(0,i.jsx)(n.span,{className:"mord mathnormal",style:{marginRight:"0.09618em"},children:"J"}),(0,i.jsx)(n.span,{className:"mopen",children:"("}),(0,i.jsx)(n.span,{className:"mord mathnormal",style:{marginRight:"0.02691em"},children:"w"}),(0,i.jsx)(n.span,{className:"mpunct",children:","}),(0,i.jsx)(n.span,{className:"mspace",style:{marginRight:"0.1667em"}}),(0,i.jsx)(n.span,{className:"mord mathnormal",children:"b"}),(0,i.jsx)(n.span,{className:"mclose",children:")"}),(0,i.jsx)(n.span,{className:"mspace",style:{marginRight:"0.2778em"}}),(0,i.jsx)(n.span,{className:"mrel",children:"="}),(0,i.jsx)(n.span,{className:"mspace",style:{marginRight:"0.2778em"}})]}),(0,i.jsxs)(n.span,{className:"base",children:[(0,i.jsx)(n.span,{className:"strut",style:{height:"1em",verticalAlign:"-0.25em"}}),(0,i.jsx)(n.span,{className:"mord mathnormal",style:{marginRight:"0.02778em"},children:"r"}),(0,i.jsx)(n.span,{className:"mord mathnormal",children:"m"}),(0,i.jsx)(n.span,{className:"mord mathnormal",children:"se"}),(0,i.jsx)(n.span,{className:"mord",children:"/"}),(0,i.jsx)(n.span,{className:"mord mathnormal",children:"m"}),(0,i.jsx)(n.span,{className:"mord mathnormal",children:"se"})]})]})]})}),"\n",(0,i.jsx)(n.li,{children:"Can be minimized with [[gradient descent]] to find the optimal parameters"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.h1,{id:"week-2-regression-with-multiple-input-variables",children:"Week 2: Regression with multiple input variables"}),"\n",(0,i.jsx)(n.h2,{id:"multiple-linear-regression",children:"Multiple Linear Regression"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Using multiple features to estimate one response variable","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Different from multivariate regression which estimates multiple response variable"}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Vectorization with ",(0,i.jsx)(n.code,{children:"numpy"})," is used to speed up the algorithm of the dot product between weights and features","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["This is done by doing the operation ",(0,i.jsx)(n.em,{children:"in parallel"})]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.li,{children:"Gradient descent works similarly but instead of taking the derivative of the cost function with respect to one feature's weight and updating one weight -> you update all the weights by taking partial derivatives of the cost function with each weight"}),"\n",(0,i.jsxs)(n.li,{children:["Just a note that an alternative solution to the linear regression is the [[normal equation]]","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"It only works for linear regression because it is a closed form solution to this"}),"\n",(0,i.jsx)(n.li,{children:"So it is not generalized solution (unlike gradient descent) and it could be slower if the features are much higher"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"gradient-descent-in-practice",children:"Gradient descent in practice"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["To make gradient descent more efficient, one should feature scale with some [[Feature Scaling Techniques]]","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"This will make the cost function contour to be evenly spaced and not be susceptible to jumping"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.h1,{id:"week-3",children:"Week 3"}),"\n",(0,i.jsx)(n.p,{children:"#machine-learning #course"})]})}function h(e={}){const{wrapper:n}={...(0,t.a)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}},11151:(e,n,s)=>{s.d(n,{Z:()=>l,a:()=>a});var i=s(67294);const t={},r=i.createContext(t);function a(e){const n=i.useContext(r);return i.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:a(e.components),i.createElement(r.Provider,{value:n},e.children)}}}]);