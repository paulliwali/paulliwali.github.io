"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[57801],{64150:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>d,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>o,toc:()=>c});var a=t(85893),i=t(11151);const r={},s="Case Studies",o={id:"slip-box/reference-notes/Data Science Practice Questions",title:"Case Studies",description:"KPI For a new product",source:"@site/docs/slip-box/reference-notes/Data Science Practice Questions.md",sourceDirName:"slip-box/reference-notes",slug:"/slip-box/reference-notes/Data Science Practice Questions",permalink:"/docs/slip-box/reference-notes/Data Science Practice Questions",draft:!1,unlisted:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/slip-box/reference-notes/Data Science Practice Questions.md",tags:[],version:"current",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Data Science Learning",permalink:"/docs/slip-box/reference-notes/Data Science Learning"},next:{title:"Deep Learning Specialization",permalink:"/docs/slip-box/reference-notes/Deep Learning Specialization"}},d={},c=[{value:"KPI For a new product",id:"kpi-for-a-new-product",level:2},{value:"AB Testing",id:"ab-testing",level:2},{value:"Generic",id:"generic",level:2},{value:"Time series ad revenue",id:"time-series-ad-revenue",level:2},{value:"Fill NAN",id:"fill-nan",level:2},{value:"Feature processing",id:"feature-processing",level:2},{value:"Regression",id:"regression",level:2}];function l(e){const n={code:"code",h1:"h1",h2:"h2",p:"p",pre:"pre",...(0,i.a)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.h1,{id:"case-studies",children:"Case Studies"}),"\n",(0,a.jsx)(n.h2,{id:"kpi-for-a-new-product",children:"KPI For a new product"}),"\n",(0,a.jsx)(n.p,{children:'Q: Discuss why meta would want to build this product, how would you set goals for this product and how do you measure success? (product rationale)\nA: At Tesla, everything we build was aligned with the mission of accelerating the world\'s transition to sustainable energy. I think that is the same with Meta, where I believe the mission is to hearken to the promise of the internet which is to connect people from all edges of the world. So the goal of the product should be "enabling the live exchange of communication between people". We should measure this in a few ways - one is regarding reach or coverage and one is regarding meaningful engagement. For both of these KPIs, I also see two sides one from the audience and one from the creator side. Maybe I would track the MAU from the creator side and the MAU (with significant threshold) from the audience side.'}),"\n",(0,a.jsx)(n.p,{children:"Follow up questions\nQ: How do you deal with cannibalization of another product\nA: If there is cannibalization, it must mean this product is filling a void that the customers want. If it doesn't compete in objective with the other product then it just might mean this is what the customer want and if we don't create it, a competitor will."}),"\n",(0,a.jsx)(n.p,{children:"Q: What if someone comes to you and says revenue metrics is bad and here is a better product but goes against your goal metrics?\nA: We should vertically align on the goal metric for the product, at Tesla a lot of our metrics are iterated over and over to reflect business and engineering needs. So that one number can clearly track the progress of the program for everyone. We can incorporate the suggested KPIs with revenue aspects."}),"\n",(0,a.jsx)(n.p,{children:"Success metrics - activation rate, retention rate, revenue growth, usage, engagement\nGuardrail metrics - bounce rate, error rate, support requests, abort rate, rate of single use over power use"}),"\n",(0,a.jsx)(n.h2,{id:"ab-testing",children:"AB Testing"}),"\n",(0,a.jsx)(n.h1,{id:"data-science",children:"Data Science"}),"\n",(0,a.jsx)(n.h2,{id:"generic",children:"Generic"}),"\n",(0,a.jsx)(n.p,{children:"Q: What is your EDA process?\nA: When conducting EDA, I first ensure I understand the data's structure and coverage. I then calculate descriptive statistics like mean, median, standard deviation, and quartiles to get a sense of the data's central tendency and variability. Next, I create visualizations like histograms, box plots, and scatter plots to understand the data's distribution, identify potential outliers, and explore relationships between variables. I also perform data quality checks, such as checking for duplicates and missing values, to ensure the data is reliable. My EDA process is flexible and adapts to the specific dataset and problem I'm trying to solve"}),"\n",(0,a.jsx)(n.p,{children:"Q: How do you handle missing data?\nA: Handling missing values depends on the specific context and application, and there's no one-size-fits-all solution. If a feature has a lot of missing values, it might be best to exclude it from the analysis to avoid biasing the model. And for sparse missing values, using the mean or nearest neighbor imputation can be effective, depending on the context. Interpolation is a great technique for time series data, where the missing values can be estimated based on the patterns and trends in the data. If the data is missing at random then imputing might be appropriate but if its not at random then more sophisticated methods or throw out might be needed."}),"\n",(0,a.jsx)(n.p,{children:"Q: Describe regularization and why it is important\nA: It prevents overfitting by penalizing the size of the coefficients or the number of coefficients. Lasso reduces all coefficients and Ridge tries to penalize the higher power ones to reduce their effects."}),"\n",(0,a.jsx)(n.p,{children:"Q: What are ensemble methods and why might they be useful?\nA: They aggregate weak models together to create a stronger model. They tend to generalize better than the individual models"}),"\n",(0,a.jsx)(n.p,{children:"Q: What is ordinal data\nA: Data that are categories but have inherent ranking to it, label encoding is useful here. Other type is nominal data, which doesn't have ranking and using one-hot encoding or binary encoding works. One would use binary encoding if the nominal data has high-cardinality since it is more compact than one-hot encoding."}),"\n",(0,a.jsx)(n.p,{children:"Q: What is target encoding\nA: To encode the categorical feature with the mean value of another target feature."}),"\n",(0,a.jsx)(n.p,{children:"Q: Name some common feature types\nA: Categorical (nominal and ordinal), numerical (continuous, discrete), binary, free text, time series (timestamps, datetime), geospatial (zip code, lat/long), derived"}),"\n",(0,a.jsx)(n.p,{children:"Q: What is the difference between normalize and standardize\nA: Normalize makes the feature between 0 and 1, standardize makes the feature to have 0 mean and 1 standard deviation"}),"\n",(0,a.jsx)(n.h2,{id:"time-series-ad-revenue",children:"Time series ad revenue"}),"\n",(0,a.jsx)(n.p,{children:"Q: What is the daily revenue generated by this product in the last 30 days in US?"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'import pandas as pd \n\ndf["ds"] = pd.to_datetime(df["ds"])\ndf_agg = df.groupby(["country_code", "ds"])[["revenue_local"]].sum()\n\ndisplay(\n\tdf_agg.loc[(df_agg["country_code"] == "US") & (df_agg["ds"] > (pd.to_datetime(\'now\').date() - pd.TimeDelta(days=30)))]\n)\n'})}),"\n",(0,a.jsx)(n.p,{children:"Q: What is the daily revenue generated every day globally"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'import pandas as pd \n\ndf_daily_fx_rates["ds"] = pd.to_datetime(df_daily_fx_rates["ds"])\n\ndf_with_currency = df.merge(df_country_currency_mapping[["country_code", "currency_code"]], how="left", on=["country_code"])\ndf_with_fx_rate = df_with_currency.merge(df_fx_rate.loc[df_fx_rate["destination_currency"] == "CAD"], how="left", left_on=["ds", "currency_code"], right_on=["ds", "source_currency"])\ndf_with_currency["revenue_cad"] = df_with_currency["revenue_local"] * df_with_currency["fx_rate"]\n\ndf_agg = df_with_currency.groupby(["country_code", "ds"])[["revenue_cad"]].sum()\n\n'})}),"\n",(0,a.jsx)(n.p,{children:"Q: When do we reach 1M USD?"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'df_with_currency.groupby("ds")[["revenue_cad"]].sum().reset_index()\ndf_with_currency["cumsum_revenue_cad"] = df_with_currency["revenue_cad"].cumsum()\n\nmin_date = df_with_currency.loc[df_with_currency["cumsum_revenue_cad"] >= 1000000, "ds"].min()\n'})}),"\n",(0,a.jsx)(n.p,{children:"Q: Assume a linear YoY growth rate, can you provide a forecasted daily revenue?\nA: One can select different ranges to compute the growth rate, should discuss some thoughts about choosing the range and the trade-offs. Using the full year is probably the safest, but won't capture any new trends that may have influenced the growth permanently."}),"\n",(0,a.jsx)(n.h2,{id:"fill-nan",children:"Fill NAN"}),"\n",(0,a.jsx)(n.p,{children:"Q: Fill NAN with the mean of the previous n values"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"data = {\n\t'user_id': [1, 1, 1, 2, 2, 2], \n\t'date': ['2022-01-01', '2022-01-02', '2022-01-04', '2022-01-01', '2022-01-02', '2022-01-03'], \n\t'likes': [10, 20, np.nan, 5, 10, np.nan]\n}\ndf = pd.DataFrame(data)\n\n# Impute missing values using SMA\nimputed_df = impute_missing_values(df, window_size=2, column_name='likes')\n"})}),"\n",(0,a.jsxs)(n.p,{children:["A: The ",(0,a.jsx)(n.code,{children:"imput_missing_values"})," function can use ",(0,a.jsx)(n.code,{children:"groupby"})," then ",(0,a.jsx)(n.code,{children:"apply..lambda"})," with a ",(0,a.jsx)(n.code,{children:"fillna"})," and ",(0,a.jsx)(n.code,{children:"rolling"})," chain. Remember to add a window size value because it includes the current value"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"def impute_missing_values(df, window_size, column_name):\n\tdf_imputed = df.groupby('user_id')[column_name].apply(\n\t  lambda x: x.fillna(x.rolling(window=window_size+1, min_periods=1).mean())\n\t)\n\n\t# do some merge to get other columns\n\n\treturn df_imputed\n"})}),"\n",(0,a.jsx)(n.p,{children:"Q: Impute missing values using median for numerical columns and mode for categorical columns\nA:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'def impute_missing(df):\n\tfor colname in df.columns:\n\t\tif df[colname].dtype == "O": # categorical \n\t\t\tdf[colname].fillna(df[colname].mode().iloc[0], inplace=True)\n\t\telif pd.api.types.is_numeric_dtype(df[colname]): # numeric\n\t\t\tdf[colname].fillna(df[colname].mean().iloc[0], inplace=True)\n\t\telse:\n\t\t\tprint("Not filling as column is neither categorical or numeric")\n\treturn df\n'})}),"\n",(0,a.jsx)(n.h2,{id:"feature-processing",children:"Feature processing"}),"\n",(0,a.jsx)(n.p,{children:"Q: Can you convert categorical features into numeric\nA:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"# Sample data\ndata = {'color': ['red', 'blue', 'green', 'red', 'blue', 'green']}\ndf = pd.DataFrame(data)\n\n# Map to a single feature using dictionary\ncolor_map = {\"red\": 0, \"blue\": 1, \"green\": 2}\ndf[\"color_mapped\"] = df[\"color\"].map(color_map)\n\n# Map to multiple columns using pandas built-in get_dummies\npd.get_dummies(df[\"color\"])\n"})}),"\n",(0,a.jsx)(n.p,{children:"Q: Can you filter for outliers\nA:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"data = [10, 12, 14, 15, 18, 20, 22, 24, 100, 200]\n\ndef detect_outliers(data):\n  df = pd.DataFrame(data, columns=[\"value\"])\n  q1 = df['value'].quantile(0.25)\n  q3 = df['value'].quantile(0.75)\n  iqr = q3 - q1\n  lower_bound = q1 - (1.5 * iqr)\n  upper_bound = q3 + (1.5 * iqr)\n\n  outliers = df.loc[(df[\"value\"] < lower_bound) | (df['value'] > upper_bound)]\n\n  return outliers\n\nprint(detect_outliers(data))\n"})}),"\n",(0,a.jsx)(n.h2,{id:"regression",children:"Regression"}),"\n",(0,a.jsx)(n.p,{children:"Q: Can you create a simple linear regression\nA: The minimization of OLS becomes coef_1 = SS_xy / SS_xx and coef_2 = y - coef_1 * x, where SS_xy is the sum of squared cross deviation and SS_xx is the sum of squared deviation"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"mean_x = np.mean(X)\nmean_y = np.mean(y)\nn = len(y)\n\nSS_xy = np.sum(y*X) - (n * mean_x * mean_y)\nSS_xx = np.sum(X**2) - (n * mean_x ** 2)\n\nb_1 = SS_xy / SS_xx\nb_0 = mean_y - b_1 * mean_x\n"})}),"\n",(0,a.jsx)(n.h1,{id:"programming",children:"Programming"}),"\n",(0,a.jsxs)(n.p,{children:["Q: How to transpose a list of lists\nA: First ensure all lists are the same length, then use ",(0,a.jsx)(n.code,{children:"zip"})," to iterate multiple elements"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"[list(row) for row in zip(*master_list)]\n"})}),"\n",(0,a.jsx)(n.p,{children:"Q: Why is a hashmap efficient\nA: Because it is index and the lookup doesn't grow with the size and is constant time"}),"\n",(0,a.jsxs)(n.p,{children:["Q: given a list of numbers, count the frequency\nA: Use ",(0,a.jsx)(n.code,{children:"Counter"})," or iterate through and update to a ",(0,a.jsx)(n.code,{children:"dict"})," with ",(0,a.jsx)(n.code,{children:"get('key', 0)"})]}),"\n",(0,a.jsx)(n.p,{children:"Q: Implement MSE\nA:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"def mse(y_true, y_pred):\n\terror = y_pred - y_true\n\tsquared_error = error ** 2\n\tmse = sum(squared_error) / len(squared_error)\n\treturn mse\n"})}),"\n",(0,a.jsx)(n.p,{children:"Q: Implement a min-max scaler\nA:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"def min_max_scaler(x_values):\n\tmin_x = min(x_values)\n\tmax_x = max(x_values)\n\n\tscaled_x = (x_values - min_x) / (max_x - min_x)\n\treturn scaled_x\n"})}),"\n",(0,a.jsx)(n.p,{children:"Q: implement cosine similarity calculation\nA: consine similarity is comparing how close two vectors are against each other, basically calculating the degree between the two vectors. Calculated by dividing the dot product between the vectors and the product of the magnitude of the two vectors. The magnitude is calculate by the Pythagorean Theorem"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python"})})]})}function u(e={}){const{wrapper:n}={...(0,i.a)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(l,{...e})}):l(e)}},11151:(e,n,t)=>{t.d(n,{Z:()=>o,a:()=>s});var a=t(67294);const i={},r=a.createContext(i);function s(e){const n=a.useContext(r);return a.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:s(e.components),a.createElement(r.Provider,{value:n},e.children)}}}]);