"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[49722],{3905:(e,t,a)=>{a.d(t,{Zo:()=>u,kt:()=>h});var i=a(67294);function n(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function r(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);t&&(i=i.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,i)}return a}function l(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?r(Object(a),!0).forEach((function(t){n(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):r(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function o(e,t){if(null==e)return{};var a,i,n=function(e,t){if(null==e)return{};var a,i,n={},r=Object.keys(e);for(i=0;i<r.length;i++)a=r[i],t.indexOf(a)>=0||(n[a]=e[a]);return n}(e,t);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(i=0;i<r.length;i++)a=r[i],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(n[a]=e[a])}return n}var s=i.createContext({}),m=function(e){var t=i.useContext(s),a=t;return e&&(a="function"==typeof e?e(t):l(l({},t),e)),a},u=function(e){var t=m(e.components);return i.createElement(s.Provider,{value:t},e.children)},p="mdxType",c={inlineCode:"code",wrapper:function(e){var t=e.children;return i.createElement(i.Fragment,{},t)}},d=i.forwardRef((function(e,t){var a=e.components,n=e.mdxType,r=e.originalType,s=e.parentName,u=o(e,["components","mdxType","originalType","parentName"]),p=m(a),d=n,h=p["".concat(s,".").concat(d)]||p[d]||c[d]||r;return a?i.createElement(h,l(l({ref:t},u),{},{components:a})):i.createElement(h,l({ref:t},u))}));function h(e,t){var a=arguments,n=t&&t.mdxType;if("string"==typeof e||n){var r=a.length,l=new Array(r);l[0]=d;var o={};for(var s in t)hasOwnProperty.call(t,s)&&(o[s]=t[s]);o.originalType=e,o[p]="string"==typeof e?e:n,l[1]=o;for(var m=2;m<r;m++)l[m]=a[m];return i.createElement.apply(null,l)}return i.createElement.apply(null,a)}d.displayName="MDXCreateElement"},50118:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>s,contentTitle:()=>l,default:()=>p,frontMatter:()=>r,metadata:()=>o,toc:()=>m});var i=a(87462),n=(a(67294),a(3905));const r={},l=void 0,o={unversionedId:"slip-box/reference-notes/Hands On Machine Learning",id:"slip-box/reference-notes/Hands On Machine Learning",title:"Hands On Machine Learning",description:"- Metadata",source:"@site/docs/slip-box/reference-notes/Hands On Machine Learning.md",sourceDirName:"slip-box/reference-notes",slug:"/slip-box/reference-notes/Hands On Machine Learning",permalink:"/docs/slip-box/reference-notes/Hands On Machine Learning",draft:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/slip-box/reference-notes/Hands On Machine Learning.md",tags:[],version:"current",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"HTTP Response Codes",permalink:"/docs/slip-box/reference-notes/HTTP Response Codes"},next:{title:"How to Become a Data Engineer",permalink:"/docs/slip-box/reference-notes/How to Become a Data Engineer"}},s={},m=[],u={toc:m};function p(e){let{components:t,...a}=e;return(0,n.kt)("wrapper",(0,i.Z)({},u,a,{components:t,mdxType:"MDXLayout"}),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},"Metadata",(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},"Source: ",(0,n.kt)("a",{parentName:"li",href:"https://github.com/ageron/handson-ml"},"https://github.com/ageron/handson-ml")),(0,n.kt)("li",{parentName:"ul"},"[","[learning]","] "))),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("h1",{parentName:"li",id:"preface"},"Preface"),(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},"The wave of machine learning is started in 2006 when [","[Geoffrey Hinton]",'] published a paper on the practicality of training a deep neural network with "Deep Learning"'),(0,n.kt)("li",{parentName:"ul"},"Will learn to use [","[Scikit-Learn]","], [","[TensorFlow]","], [","[Keras]","]"),(0,n.kt)("li",{parentName:"ul"},"Roadmap of the book",(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},"Part I - The Fundamentals of Machine Learning (focused on [","[Scikit-Learn]","])",(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},"What ML is"),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("strong",{parentName:"li"},"Steps to a typical ML project")),(0,n.kt)("li",{parentName:"ul"},"Cost function and how to optimize it"),(0,n.kt)("li",{parentName:"ul"},"Data wrangling"),(0,n.kt)("li",{parentName:"ul"},"Feature selection and engineering"),(0,n.kt)("li",{parentName:"ul"},"(Hyper)parameter tuning, model selection via cross-validation"),(0,n.kt)("li",{parentName:"ul"},"Challenges of ML"),(0,n.kt)("li",{parentName:"ul"},"Common/Traditional ML algorithms"),(0,n.kt)("li",{parentName:"ul"},"Reducing dimensionality"),(0,n.kt)("li",{parentName:"ul"},"Unsupervised ML algorithms"))),(0,n.kt)("li",{parentName:"ul"},"Part II - Neural networks and deep learning (focused on [","[TensorFlow]","] and [","[Keras]","])",(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},"What neural nets are"),(0,n.kt)("li",{parentName:"ul"},"Build and train NN with [","[TensorFlow]","] and [","[Keras]","]"),(0,n.kt)("li",{parentName:"ul"},"Important NN architectures "),(0,n.kt)("li",{parentName:"ul"},"Techniques for training deep NN"),(0,n.kt)("li",{parentName:"ul"},"RL to build a bot"),(0,n.kt)("li",{parentName:"ul"},"Loading and preprocessing big data"),(0,n.kt)("li",{parentName:"ul"},"Training and deploying [","[TensorFlow]","] at scale"))))))),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("h1",{parentName:"li",id:"chapter-1-the-machine-learning-landscape"},"Chapter 1. The Machine Learning Landscape"),(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},"Machine learning is to improve the ability of doing a certain task with experience, measured by a performance metric"),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("img",{parentName:"li",src:"https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/assets/mls2_0104.png",alt:"mls2 0104"})),(0,n.kt)("li",{parentName:"ul"},"Main types of ML",(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},"Whether they are trained under human supervision (supervised vs unsupervised)"),(0,n.kt)("li",{parentName:"ul"},"Whether they can learn incrementally on the fly (online vs batch learning)"),(0,n.kt)("li",{parentName:"ul"},"Whether they work by comparing data points or detecting patterns in the data (instance based or model based)"))),(0,n.kt)("li",{parentName:"ul"},"The segmentation for human supervision are",(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},"Supervised learning",(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},"Training data includes labels, which are the ground truths"),(0,n.kt)("li",{parentName:"ul"},"These are good for classification problems (spam vs ham) or regression problems (price of house given a set of features)"))),(0,n.kt)("li",{parentName:"ul"},"Unsupervised learning",(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},"Training data ",(0,n.kt)("strong",{parentName:"li"},"does not")," have labels"),(0,n.kt)("li",{parentName:"ul"},"These are good for detecting patterns like clustering problems (k means) or anomaly detection or dimensionality reduction or association rule learning problems"),(0,n.kt)("li",{parentName:"ul"},"Dimensionality reduction is an important technique in ML, that simplify the data without losing information - several features can merge into one because they are related (feature extraction)"),(0,n.kt)("li",{parentName:"ul"},"Association rule learning is to learn relationships between attributes from data",(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},"Supermarket putting two items in the same isle to promote sales"))))),(0,n.kt)("li",{parentName:"ul"},"Semisupervised learning",(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},"Labelling all the data is expensive, so you can combine labelled and unlabelled data together to partially labelled data"))),(0,n.kt)("li",{parentName:"ul"},"Reinforcement learning",(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},"An agent observes the environment and makes an action, which the system rewards or penalizes"),(0,n.kt)("li",{parentName:"ul"},"Using this feedback, the agent develops a strategy (policy) to maximize rewards over time"))))),(0,n.kt)("li",{parentName:"ul"},"Learning from training data can be done all at once or incrementally",(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},"Batch learning",(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},"Learn from all the data and use the model in application"),(0,n.kt)("li",{parentName:"ul"},"Good when data is not continuous"))),(0,n.kt)("li",{parentName:"ul"},"Online learning",(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},"Learn data as small batches"),(0,n.kt)("li",{parentName:"ul"},"Good when resources are limited and data is constantly streaming in"),(0,n.kt)("li",{parentName:"ul"},"Beware of learning from bad data, close monitoring is required. Also optimizing learning rate to carry inertia or react to new data requires thought"),(0,n.kt)("li",{parentName:"ul"},"==out-of-core== learning is to artificially break large data sets into small batches and trained with this method"))))),(0,n.kt)("li",{parentName:"ul"},"How well the model generalize the training into application",(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},"Instance-based learning",(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},"Makes predictions by comparing them with examples in the data"))),(0,n.kt)("li",{parentName:"ul"},"Model-based learning",(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},"Makes predictions with a model"))))),(0,n.kt)("li",{parentName:"ul"},"Main Challenges of ML",(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},"Bad data",(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},"Michele Banko and Eric Brill (Microsoft) published a ",(0,n.kt)("a",{parentName:"li",href:"https://dl.acm.org/doi/10.3115/1073012.1073017"},"paper")," in 2001 that showed that if given enough data the simple models perform just as well as complex ones"),(0,n.kt)("li",{parentName:"ul"},"Small datasets have ",(0,n.kt)("strong",{parentName:"li"},"sampling noise")," and ",(0,n.kt)("strong",{parentName:"li"},"sampling bias"),(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},"An famous example of sampling bias was during the 1936 US election, Literary Digest conduct a very large survey on the voter's preference for Landon and Roosevelt",(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},"The sampling method was flawed because the sampling population was mostly wealthier who favoured Landon (Republican)"),(0,n.kt)("li",{parentName:"ul"},"Less than 25% of the original population answered, introducing ",(0,n.kt)("strong",{parentName:"li"},"nonresponse bias")))))),(0,n.kt)("li",{parentName:"ul"},"Cleaning up training data by removing outliers and incomplete data may help with training"),(0,n.kt)("li",{parentName:"ul"},"Using irrelevant features "))),(0,n.kt)("li",{parentName:"ul"},"Bad algorithm",(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},"Overfitting on training data and coming to a general conclusion that is not true about the population"),(0,n.kt)("li",{parentName:"ul"},"NN will be able to detect subtle relationships in data, but any noise or bias may cause it to capture these relationships incorrectly"),(0,n.kt)("li",{parentName:"ul"},"Under-fitting on training data will also lead to false conclusions about the population"))))),(0,n.kt)("li",{parentName:"ul"},"Testing and validation",(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},"Using a test sample to evaluate the performance of the algorithm by looking at the generalized or out-of-sample error rate",(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},"If training error is low but out-of-sample error is high, then the model has overfitted the data"))),(0,n.kt)("li",{parentName:"ul"},"It is also common to tune hyperparameters to reduce this out-of-sample error but now you have just adapted the model to the testing data",(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},"==So it is common to have another hold-out set for validation=="),(0,n.kt)("li",{parentName:"ul"},'However, since there may not be enough data to do all three tasks - a technique called "cross-validation" is used to shuffle training/test/validation sets so the hyperparameters can be tuned and then the final set of hyperparameters is used on the training set and validated on the validation set'))),(0,n.kt)("li",{parentName:"ul"},"[","[til2021]","] No Free Lunch Theorem",(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},"David Wolpert in a famous paper (1996) showed that if you make no assumptions about the data, there is no reason to choose one model over another"),(0,n.kt)("li",{parentName:"ul"},"There is no way to know ahead of time that one model will work better than another"),(0,n.kt)("li",{parentName:"ul"},"Thus, to practically build models you have to make some assumptions"))))))),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("h1",{parentName:"li",id:"chapter-2-end-to-end-machine-learning-project"},"Chapter 2: End to End Machine Learning Project"),(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},"Hands on project to go through the common steps of build a ML model"),(0,n.kt)("li",{parentName:"ul"},"To measure the performance of a model common metrics are RMSE and MAE, ",(0,n.kt)("strong",{parentName:"li"},"which measures the distance between prediction and actual"),(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},"Root mean squared error (RMSE)",(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},"$$\\sqrt{\\frac{1}{m} \\sum{\\hat{y} - y}}$$"),(0,n.kt)("li",{parentName:"ul"},"Standard metric, but is prone to outliers"))),(0,n.kt)("li",{parentName:"ul"},"Mean absolute error (MAE)",(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},"$$\\frac{1}{m}\\sum{|\\hat{y} - y|}$$"))))),(0,n.kt)("li",{parentName:"ul"},"Data preparation: Splitting training, test and validation set is critical to not overtrain the model",(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},"Naively splitting doesn't work because every time you split you may end up with different splits and that doesn't give reproducible results and the algorithm will see the entire dataset if it is continuously trained"),(0,n.kt)("li",{parentName:"ul"},"Sampling bias might be introduced if the distribution of the respondents is not representative of the general population, so ",(0,n.kt)("strong",{parentName:"li"},"stratified sampling")," is used to overcome this"))),(0,n.kt)("li",{parentName:"ul"},"Data exploration: correlation coefficients, matrix",(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("inlineCode",{parentName:"li"},"corr()")," simply calculates the ",(0,n.kt)("strong",{parentName:"li"},"linear correlation")," between various features"),(0,n.kt)("li",{parentName:"ul"},"Ranges from -1 and 1 which denotes negative correlation and positive correlation"),(0,n.kt)("li",{parentName:"ul"},"A value close to 0 means no linear correlation, ==which doesn't rule out other correlations=="),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("img",{parentName:"li",src:"https://en.wikipedia.org/wiki/File:Correlation_examples2.svg",alt:"pearson's correlation figure"})),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("strong",{parentName:"li"},"The goal here is to identify data quirks to remove/clean before feeding into the training")))),(0,n.kt)("li",{parentName:"ul"},"Feature engineering: cleaning, transforming, combining attributes",(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},"Feature cleaning involves removing empty values from the dataset",(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},"There are three options for missing features: remove the data entry, remove the feature, or set values to median, mean or zero"),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("strong",{parentName:"li"},"It is important to do this only for the training set"),(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},"For mean/median calculation, they should be computed on the training set but also applied to the test set when testing the accuracy"))),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("inlineCode",{parentName:"li"},"sklearn.impute.SimpleImputer")," takes care of imputing missing values"))),(0,n.kt)("li",{parentName:"ul"},"Feature generation involves combining features into more useful ones such as normalizing certain features with another to give more context "),(0,n.kt)("li",{parentName:"ul"},"Feature transformation involves changing features into different forms that is suitable for ML",(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},"Convert text categories into numbers",(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("inlineCode",{parentName:"li"},"sklearn.preprocessing.OrdinalEncoder")," which turns text into ranked list, but ML will assume two close by values are similar"),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("inlineCode",{parentName:"li"},"sklearn.preprocessing.OneHotEncoder")," which turns text in to dummy attributes and doesn't imply close by values matter"))),(0,n.kt)("li",{parentName:"ul"},"If one-hot encoding is resulting in a large number of input features and slowing down ML performance, ==you can replace categorical input with useful numerical features==",(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},"i.e. Country code can be proxied with GDP or population"),(0,n.kt)("li",{parentName:"ul"},"i.e. Ocean proximity can actually be distance in meters"),(0,n.kt)("li",{parentName:"ul"},"==Or you can replace each category with a learnable low dimensional vector called an embedding=="))))),(0,n.kt)("li",{parentName:"ul"},"Feature scaling shifts the values in a feature for ML",(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("strong",{parentName:"li"},"Usually, ML doesn't perform well on datasets with features in various scales"),". "),(0,n.kt)("li",{parentName:"ul"},"Min-max scaling (aka ",(0,n.kt)("strong",{parentName:"li"},"normalization"),") changes values such that it is between two values, most commonly 0 and 1"),(0,n.kt)("li",{parentName:"ul"},"Standardization scaling changes the values such that the mean is 0 and distribution has a unit variance"))))),(0,n.kt)("li",{parentName:"ul"},"[","[Scikit-Learn]","] - Core API design principles"),(0,n.kt)("li",{parentName:"ul"},"Select and Train a Model",(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},"Linear regression model",(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},"A good baseline model to compare others too"),(0,n.kt)("li",{parentName:"ul"},"Might underfitting the data"),(0,n.kt)("li",{parentName:"ul"},"Main ways to fix under-fitting is selecting a more powerful model, to feed the training algorithm with better features, or to reduce the constraints on the model."))),(0,n.kt)("li",{parentName:"ul"},"Decision Tree Regressor",(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},"A powerful model that is capable of finding nonlinear relationships in data"),(0,n.kt)("li",{parentName:"ul"},"Easy to overfit, so use cross-validation to evaluate the models"))),(0,n.kt)("li",{parentName:"ul"},"Random Forest Regressor",(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},"Training many decision trees on random subsets of the feature, then averaging out their predictions"),(0,n.kt)("li",{parentName:"ul"},"A kind of ",(0,n.kt)("strong",{parentName:"li"},"ensemble learning")," which is building a model on top of many other models"))),(0,n.kt)("li",{parentName:"ul"},"Others to try are Support Vector Machines or even Neural Networks"),(0,n.kt)("li",{parentName:"ul"},"At this stage, one should aim to shortlist a few high potential models ",(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},"To save their hyperparameters and trained parameters we can use ",(0,n.kt)("inlineCode",{parentName:"li"},"pickle")," or ",(0,n.kt)("inlineCode",{parentName:"li"},"joblib")," from sklearn"))))),(0,n.kt)("li",{parentName:"ul"},"Fine Tune Your Model",(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},"Once you have a shortlist of models, we need to fine tune each one though grid search, randomized search, ensemble methods"),(0,n.kt)("li",{parentName:"ul"},"Grid Search",(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},"Programmatically search through all the hyperparameter combinations using ",(0,n.kt)("inlineCode",{parentName:"li"},"GridSerachCV")),(0,n.kt)("li",{parentName:"ul"},"When you don't have a good sense of the grid to search with, a good approach is consecutive powers of 10"),(0,n.kt)("li",{parentName:"ul"},"Once the grid search is done, find the best combination of parameters by ",(0,n.kt)("inlineCode",{parentName:"li"},"grid_serach.best_params_")," and see if they are the upper limit of the grid you setup",(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},"Try searching again if they are the maximum values in the grid"))))),(0,n.kt)("li",{parentName:"ul"},"Randomized Search",(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},"Grid search is a good option when the space is limited, but if the search space is large then we can use ",(0,n.kt)("inlineCode",{parentName:"li"},"RandomizedSearchCV")," instead"),(0,n.kt)("li",{parentName:"ul"},"It can search for more combinations with less cost"))),(0,n.kt)("li",{parentName:"ul"},"Ensemble Methods",(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},"Combining models that perform the best since a group of model will often perform better than the best individual model"),(0,n.kt)("li",{parentName:"ul"},"Analyze the best model by inspecting their feature importance and dropping the ones that is less important"))))),(0,n.kt)("li",{parentName:"ul"},"Evaluate Final Model on Test Set",(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},"Prepare the reserved test data set the same way and fit it through the estimator"),(0,n.kt)("li",{parentName:"ul"},"Be sure to only ",(0,n.kt)("inlineCode",{parentName:"li"},"transform()")," and not ",(0,n.kt)("inlineCode",{parentName:"li"},"fit_transform()")),(0,n.kt)("li",{parentName:"ul"},"Also generalize the error in production with a confidence interval to get a sense of the variations in the error"),(0,n.kt)("li",{parentName:"ul"},"Performance will usually be slightly worse on test set than on training set, but not always"))),(0,n.kt)("li",{parentName:"ul"},"Launch, Monitor and Maintain Your Model/System",(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},"Plugging the production input data into the system and writing tests"),(0,n.kt)("li",{parentName:"ul"},"Write monitoring code to check your system's live performance at regular intervals and trigger alerts when it drops",(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},"Models tend to 'rot' as data evolves over time (or drift)"))),(0,n.kt)("li",{parentName:"ul"},"Evaluating your system's performance will require sampling the system's prediction and evaluating them"),(0,n.kt)("li",{parentName:"ul"},"Evaluate the systems input data quality"),(0,n.kt)("li",{parentName:"ul"},"Retrain the model on a regular basis using fresh data"))))),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("h1",{parentName:"li",id:"chapter-3-classification"},"Chapter 3: Classification"),(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},'MNIST dataset is the "Hello World" of ML, a dataset of handwritten digits'),(0,n.kt)("li",{parentName:"ul"},"Scikit-Learn provides this dataset as part of the library ",(0,n.kt)("inlineCode",{parentName:"li"},"fetch_openml('mnist_784', version=1)"),(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},"Datasets from Scikit-Learn generally have a similar dictionary structure that includes ",(0,n.kt)("inlineCode",{parentName:"li"},"DESCR")," key describing the dataset, ",(0,n.kt)("inlineCode",{parentName:"li"},"data")," key containing an array with one row per instance and one column per feature, ",(0,n.kt)("inlineCode",{parentName:"li"},"target")," key containing an array with the labels"))),(0,n.kt)("li",{parentName:"ul"},"Binary Classifier",(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},"==Able to identify one thing by distinguishing between that thing and not-that-thing=="),(0,n.kt)("li",{parentName:"ul"},"Scikit's Stochastic Gradient Descent (SGD) classifier is a good place to start",(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},"It can handle large datasets efficiently"),(0,n.kt)("li",{parentName:"ul"},"Also suited for online training because it deals with one instance at a time"),(0,n.kt)("li",{parentName:"ul"},"It relies on randomness and shuffled data so it is not suited for any time-series analysts"))))),(0,n.kt)("li",{parentName:"ul"},"Evaluating Classifiers",(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},"A trickier subject compared to evaluating regressors"),(0,n.kt)("li",{parentName:"ul"},"Cross-validation",(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},"Similar to how we evaluated the regression model "),(0,n.kt)("li",{parentName:"ul"},"But can be problematic on ",(0,n.kt)("strong",{parentName:"li"},"skewed datasets")))),(0,n.kt)("li",{parentName:"ul"},"Confusion matrix",(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},"A better way to examine a classifier"),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("strong",{parentName:"li"},"Precision")," is the measure of how many true positives were predicted out of all the predicted positives",(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},"$$Precision = \\frac{TP}{TP + FP}$$"))),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("strong",{parentName:"li"},"Recall")," is the measure of how many true positives were predicted out of all the actual positives",(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},"$$Recall = \\frac{TP}{TP + FN}$$"))),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("strong",{parentName:"li"},"F1 Score")," is the combined metric of precision and recall. This will favor model that balances both metrics however that may not always be true"),(0,n.kt)("li",{parentName:"ul"},"Precision/Recall Tradeoff",(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},"When you increase precision, you will decrease recall"),(0,n.kt)("li",{parentName:"ul"},"This is because a classifier like SGDClassifier has a decision function that determines the threshold of where to assign the positive and negative instances"),(0,n.kt)("li",{parentName:"ul"},"To determine what the ideal threshold for the decision function, we can plot the ",(0,n.kt)("strong",{parentName:"li"},"precision recall curve")))),(0,n.kt)("li",{parentName:"ul"},"ROC Curve",(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},"The receiver operating characteristic curve is another way to tune the classifier"),(0,n.kt)("li",{parentName:"ul"},"It plots the ",(0,n.kt)("inlineCode",{parentName:"li"},"true-positive rate (recall)")," against ",(0,n.kt)("inlineCode",{parentName:"li"},"false-positive rate"),(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},"False-positive rate is ",(0,n.kt)("inlineCode",{parentName:"li"},"1 - specificity (true-negative rate)")))),(0,n.kt)("li",{parentName:"ul"},"Thus, it plots sensitivity (recall) against 1 - specificity"),(0,n.kt)("li",{parentName:"ul"},"Usually it is plotted against a diagonal which represents a random classifier and ",(0,n.kt)("strong",{parentName:"li"},"good classifiers will be as far away from the diagonal as possible")),(0,n.kt)("li",{parentName:"ul"},"The area under the curve (AUC) is another metric to quantify the performance, a perfect classifier will have an AUC of 1.0 and random classifier will have an AUC of 0.5"))),(0,n.kt)("li",{parentName:"ul"},"==Use PR curve when positive class is rare and you care about false positives, and ROC curve when negative class is rare=="))),(0,n.kt)("li",{parentName:"ul"},"Error analysis",(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},"After going through the machine learning checklist you arrive at the fine tuned model, you want to understand what kind of error it is producing"),(0,n.kt)("li",{parentName:"ul"},"Use the confusion matrix"))))),(0,n.kt)("li",{parentName:"ul"},"Multiclass Classifier",(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},"Distinguishes between many classes"),(0,n.kt)("li",{parentName:"ul"},"These can be native multiclass classifiers like RandomForestClassifier or naive Bayes classifier, or multiple binary classifiers can be combined to do this like Support Vector Machine classifier or Vector Classifier"),(0,n.kt)("li",{parentName:"ul"},"One-versus-all (OvA) strategy",(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},"Train n binary classifiers for each of the n classes"),(0,n.kt)("li",{parentName:"ul"},"Pick the classifier that gives the highest score"))),(0,n.kt)("li",{parentName:"ul"},"One-versus-one (OvO) strategy",(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},"Train $$n \\times \\frac{n-1}{2}$$ binary classifiers for each pair of classes"),(0,n.kt)("li",{parentName:"ul"},"The benefit of this strategy is that you only need to train on data with the two classes"),(0,n.kt)("li",{parentName:"ul"},"The disadvantage is that it scales non-linearly as the number of classes increases"))),(0,n.kt)("li",{parentName:"ul"},"OvO is preferred with classifiers that are more efficient on smaller datasets (SVM)while OvA is preferred most other binary classifiers",(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},"Scikit-learn will automatically optimize the strategy for the classifier"))))),(0,n.kt)("li",{parentName:"ul"},"Multilabel Classifier",(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},"Provide multiple outputs for an input"),(0,n.kt)("li",{parentName:"ul"},"Evaluating this classifier can be measuring F1 score for each individual label then compute the average score"))),(0,n.kt)("li",{parentName:"ul"},"Multioutput Classifier",(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},"Generalizes the multiclass classification where each label can be multiclass"),(0,n.kt)("li",{parentName:"ul"},"A example would be a system that removes noise from images which means it outputs multiple labels with various values for each label"))))),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("h1",{parentName:"li",id:"chapter-4-training-models"},"Chapter 4: Training Models"),(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},"Understanding how the models work under the hood"),(0,n.kt)("li",{parentName:"ul"},"[","[Linear regression model]","]",(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},"There are two ways to train linreg models"),(0,n.kt)("li",{parentName:"ul"},"A closed-form [","[normal equation]","] to compute parameters that best fit the model to the training set and ",(0,n.kt)("strong",{parentName:"li"},"iterative optimization approach [","[gradient descent]","] that gradually tweaks the parameters to minimize the cost function over the training set")))),(0,n.kt)("li",{parentName:"ul"},"[","[Polynomial regression model]","]",(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},"More complex as it can fit non-linear dataset and since it has more parameters than linreg it is prone to ",(0,n.kt)("strong",{parentName:"li"},"overfitting")," the training set"),(0,n.kt)("li",{parentName:"ul"},"To overcome this, we will detect this using [","[learning curves]","] and reduce the risk of overfitting with [","[Regularization Techniques]","] "))),(0,n.kt)("li",{parentName:"ul"},"Machine learning model's errors can be expressed as the sum of three different errors",(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},"==Bias==: from wrong assumptions"),(0,n.kt)("li",{parentName:"ul"},"==Variance==: from excessive sensitivity to small variations in the data, likely from high degrees of freedom"),(0,n.kt)("li",{parentName:"ul"},"==Irreducible errors==: from noise in the data itself, fixed by cleaning"),(0,n.kt)("li",{parentName:"ul"},"There is a tradeoff between these three error sources, increasing a model's complexity will increase variance and reduce bias and vice versa"))),(0,n.kt)("li",{parentName:"ul"},"Logistic regression model",(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},"To estimate probability that an instance belongs to a particular class"),(0,n.kt)("li",{parentName:"ul"},"Works by computing the weighted sum of the features (+ bias term) and apply the ",(0,n.kt)("strong",{parentName:"li"},"logistic"),"  to the result",(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},"A ",(0,n.kt)("strong",{parentName:"li"},"sigmoid function")," that produces a number between 0 and 1"),(0,n.kt)("li",{parentName:"ul"},"$$\\sigma(t) = \\frac{1}{1+exp(-t)}$$"),(0,n.kt)("li",{parentName:"ul"},"logit is the inverse of the logistic"))),(0,n.kt)("li",{parentName:"ul"},"To train a logistic model we have to use gradient descent because there is no close form equation"),(0,n.kt)("li",{parentName:"ul"},"Can also be regularized to prevent overfitting"))),(0,n.kt)("li",{parentName:"ul"},"Softmax regression model",(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},"Also known as ",(0,n.kt)("strong",{parentName:"li"},"multinomial logistic regression"),", which supports multiple classes"),(0,n.kt)("li",{parentName:"ul"},"Applies a normalized exponential equation (softmax) to the score of each class",(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},"The score is computed like a linear regression"),(0,n.kt)("li",{parentName:"ul"},"Softmax equation: $$\\hat{p}=\\frac{exp(s",(0,n.kt)("em",{parentName:"li"},"k(x))}{\\sum"),"{j=1}^{K}=exp(s_j(x))}$$",(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},"Where $$K$$ is the number of classes, $$s(x)$$ is the vector of the scores for each class, $$\\hat{p}$$ is the estimated probability that the instance x belongs to k "))))))))),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("h1",{parentName:"li",id:"chapter-5-support-vector-machines"},"Chapter 5: Support Vector Machines"),(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},"SVM is capable of linear and nonlinear classification, regression and outlier detection",(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},"One of the most popular ML models"))),(0,n.kt)("li",{parentName:"ul"},"Linear SVM Classification",(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},"Fitting the largest possible gap between two classes with a ",(0,n.kt)("strong",{parentName:"li"},"straight line")),(0,n.kt)("li",{parentName:"ul"},"The ",(0,n.kt)("strong",{parentName:"li"},"support")," comes from the instances located on the edge of this gap",(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},"Adding more instances outside will not affect the decision boudnary"))),(0,n.kt)("li",{parentName:"ul"},"==Very sensitive to scale, so always be sure to feature scale the dataset for linear SVM classification=="))),(0,n.kt)("li",{parentName:"ul"},"Soft Margin Classification",(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},"A more flexible model that allows for instances that end up in the boundary or on the wrong side"))),(0,n.kt)("li",{parentName:"ul"},"Nonlinear SVM Classification",(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},"When a dataset is not linearly separable on one feature, you can introduce more features to result in a linearly separable dataset"),(0,n.kt)("li",{parentName:"ul"},"There are a few ways to add more features",(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},"Polynomial Kernel",(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},"==A way to add many polynomial features without actually adding them so the feature set does not explode=="))),(0,n.kt)("li",{parentName:"ul"},"Adding Similarity Features",(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},"Measures how much each instance resembles a particular ",(0,n.kt)("strong",{parentName:"li"},"landmark")),(0,n.kt)("li",{parentName:"ul"},"Similarity can be measured with the ",(0,n.kt)("strong",{parentName:"li"},"Gaussian Radial Bias Function (RBF)")),(0,n.kt)("li",{parentName:"ul"},"And when the features are transformed to the similarity values, the instances become linearly separable "))))))),(0,n.kt)("li",{parentName:"ul"},"SVM Regression",(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},"Instead of fitting a gap ",(0,n.kt)("strong",{parentName:"li"},"between")," the instances, you fit the ",(0,n.kt)("strong",{parentName:"li"},"gap")," to cross the most instances"))))),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("h1",{parentName:"li",id:"chapter-6-decision-trees"},"Chapter 6: Decision Trees"),(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},"Can perform both classification and regression modelling"),(0,n.kt)("li",{parentName:"ul"},"Building blocks of Chapter 7: Random Forests "),(0,n.kt)("li",{parentName:"ul"},"To make a prediction with decision tree you go through nodes and you move down the tree's nodes until you reach the final node that gives the prediction"),(0,n.kt)("li",{parentName:"ul"}))),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("h1",{parentName:"li",id:"chapter-7-random-forests"},"Chapter 7: Random Forests"))))}p.isMDXComponent=!0}}]);