"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[80632],{28453:(e,t,a)=>{a.d(t,{R:()=>o,x:()=>r});var s=a(96540);const n={},i=s.createContext(n);function o(e){const t=s.useContext(i);return s.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function r(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(n):e.components||n:o(e.components),s.createElement(i.Provider,{value:t},e.children)}},43272:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>c,contentTitle:()=>r,default:()=>d,frontMatter:()=>o,metadata:()=>s,toc:()=>h});const s=JSON.parse('{"id":"slip-box/writing-bin/Making Sense of the Data Science Stack","title":"Making Sense of the Data Science Stack","description":"As someone who learned data science on the fly with each job, I am always confused about all the different layers of technology a typical data science stack is comprised of.","source":"@site/docs/slip-box/writing-bin/Making Sense of the Data Science Stack.md","sourceDirName":"slip-box/writing-bin","slug":"/slip-box/writing-bin/Making Sense of the Data Science Stack","permalink":"/docs/slip-box/writing-bin/Making Sense of the Data Science Stack","draft":false,"unlisted":false,"editUrl":"https://github.com/paulliwali/paulliwali.github.io/tree/main/docs/docs/slip-box/writing-bin/Making Sense of the Data Science Stack.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"tyranny of convenience","permalink":"/docs/slip-box/reference-notes/tyranny of convenience"},"next":{"title":"\u25f6 Trackers","permalink":"/docs/\u25f6 Trackers"}}');var n=a(74848),i=a(28453);const o={},r=void 0,c={},h=[];function l(e){const t={a:"a",p:"p",...(0,i.R)(),...e.components};return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(t.p,{children:"As someone who learned data science on the fly with each job, I am always confused about all the different layers of technology a typical data science stack is comprised of.\nThere are so many interchanging parts and sub-features replacements it is always a spaghetti mess in my head.\nJust look at this diagram!\nWorking as data scientist full time for that past few months, I hope to straighten some concepts out in my head."}),"\n",(0,n.jsx)(t.p,{children:"The most straight forward analogy that I have in my mind is mapping the layers to a traditional warehouse system with objects, containers, cabinets, clerks, and supervisor.\nAnd to run this warehouse efficiently you want each component of the system to be compatible with each other and suited for the type of item your warehouse is storing."}),"\n",(0,n.jsxs)(t.p,{children:["Starting from the lowest level with the objects themselves, this layer deals with how raw data is stored.\nData storage for a warehouse can be in various forms like seeds in a vault, vials of samples in a hospital paper and forms for medical records at the dentist, or copies of receipts at a restaurant.\nDepending on the frequency of need to retrieve and read the data or the size consideration of the data there are some trade-offs between the most common technology used in this layer: CSV, JSON, parquet.\nThe pros and cons of these file format is the trade-off between human usability and computation performance.\nIn general, if the file format is easy for you to read and edit then it is probably not very computationally efficient for the overall data stack.\nA good breakdown of these three file types ",(0,n.jsx)(t.a,{href:"https://luminousmen.com/post/big-data-file-formats",children:"article"})]}),"\n",(0,n.jsx)(t.p,{children:'There is an additional consideration that comes with data redundancy and that is how many copies should the warehouse store?\nIn the Hadoop ecosystem (one of the most popular data science environments) the de facto system is HDFS or Hadoop Distributed File System.\nThe other popular alternative is S3 or Amazon\'s Simple Storage System.\nHDFS was built for fault tolerance by distributing the files across multiple machines automatically so if one machine dies or files becomes corrupted there are always backups.\nSo imagine several filing cabinets with the same information.\nS3 was built for scalability by storing objects with keys, kind of like each cabinet stores a "bucket" of information.\nUsually people prefer to use HDFS for on-prem use cases because it allows horizontal scaling by adding more machines as opposed to vertical scaling by replacing servers with better ones\nBut because S3 is an object storage it is a bit like a binder with lots of pages inside compared to HDFS which is like lots of folders good labelling. It makes the search process much faster'}),"\n",(0,n.jsx)(t.p,{children:"Once we know the data is being stored we need to organize it in a way that it is easy to access, kind of like a library with good signs that helps the reader to head towards the right direction.\nAn example technology that is used for this is Hive Metastore, others include Apache Iceberg, AWS Glue\nThese basically organize the data into a structured way that users can more easily navigate"}),"\n",(0,n.jsx)(t.p,{children:"Then moving up to data retrieval, probably the most important aspect of the stack.\nThis layer is analogous to the workers or clerks physically finding and retrieving the items.\nIndividual workers probably has some way of looking up the files they need, maybe they retrieve in order of what they need, maybe the categorize the requests first so they can retrieve all the files in the same cabinet.\nThese different kinds of data retrieval methods are similar to Spark Core and Mapreduce"}),"\n",(0,n.jsx)(t.p,{children:"Lastly, there is supervisor that is running the warehouse, they assign schedules to the workers to ensure that they meet the demand and are not understaffed.\nThis is equivalent to YARN or Mesos which are resource management technologies."}),"\n",(0,n.jsx)(t.p,{children:"All together these layers form a typical data warehouse stack that many companies and data scientists use."})]})}function d(e={}){const{wrapper:t}={...(0,i.R)(),...e.components};return t?(0,n.jsx)(t,{...e,children:(0,n.jsx)(l,{...e})}):l(e)}}}]);