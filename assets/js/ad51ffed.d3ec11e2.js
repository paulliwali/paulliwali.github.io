"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[2318],{26081:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>r,metadata:()=>a,toc:()=>c});var s=t(85893),i=t(11151);const r={source:["https://explainextended.com/2023/12/31/happy-new-year-15/"],author:["Quassnoi"],tags:["machine-learning"]},o="\ud83d\udcf0 Summary (use your own words)",a={id:"slip-box/reference-notes/GPT in SQL",title:"\ud83d\udcf0 Summary (use your own words)",description:"Explains GPT with a SQL demo",source:"@site/docs/slip-box/reference-notes/GPT in SQL.md",sourceDirName:"slip-box/reference-notes",slug:"/slip-box/reference-notes/GPT in SQL",permalink:"/docs/slip-box/reference-notes/GPT in SQL",draft:!1,unlisted:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/slip-box/reference-notes/GPT in SQL.md",tags:[{label:"machine-learning",permalink:"/docs/tags/machine-learning"}],version:"current",frontMatter:{source:["https://explainextended.com/2023/12/31/happy-new-year-15/"],author:["Quassnoi"],tags:["machine-learning"]},sidebar:"tutorialSidebar",previous:{title:"Functional Data Engineering",permalink:"/docs/slip-box/reference-notes/Functional Data Engineering"},next:{title:"Gaussian Mixture Models",permalink:"/docs/slip-box/reference-notes/Gaussian Mixture Models"}},l={},c=[{value:"Tokenizer",id:"tokenizer",level:3},{value:"Embedding",id:"embedding",level:3},{value:"Attention",id:"attention",level:3}];function d(e){const n={code:"code",em:"em",h1:"h1",h3:"h3",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.a)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.h1,{id:"-summary-use-your-own-words",children:"\ud83d\udcf0 Summary (use your own words)"}),"\n",(0,s.jsx)(n.p,{children:"Explains GPT with a SQL demo"}),"\n",(0,s.jsx)(n.h1,{id:"\ufe0f-notes",children:"\u270d\ufe0f Notes"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:['A generative LLM is a function that takes in some texts (prompt) and returns strings and numbers which represents the probability this string "should" be next',"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["The function is ",(0,s.jsx)(n.strong,{children:"deterministic"})]}),"\n",(0,s.jsx)(n.li,{children:"But how can GPTs give different answers?"}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.li,{children:"The family of algorithms that found success in creating LLMs with good grammar and structure is called generative pre-trained transformers"}),"\n",(0,s.jsx)(n.li,{children:"The rough structure of a GPT is as follows"}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"def generate(prompt: str) -> str:\n\ttokens = tokenize(prompt)\n\n\twhile True:\n\t\tcandidates = gpt2(tokens)\n\t\tnext_token = select_next_token(candidates)\n\n\t\ttokens.append(next_token)\n\n\t\tif should_stop_generating():\n\t\t\tbreak\n\n\tcomletion = detokenize(tokens)\n\treturn completion\n"})}),"\n",(0,s.jsx)(n.h3,{id:"tokenizer",children:"Tokenizer"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"converting from strings to numbers in an efficient way"}),"\n",(0,s.jsxs)(n.li,{children:["byte-pair encoding was what was used in GPT2","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"start with encoding of 256 tokens, one for each byte"}),"\n",(0,s.jsx)(n.li,{children:"then find the most common 2-token combinations"}),"\n",(0,s.jsx)(n.li,{children:"assign the encoding of this to the next token, repeat"}),"\n",(0,s.jsx)(n.li,{children:"do this x times"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"embedding",children:"Embedding"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Each token has orthogonal properties which can be encoded with vectors"}),"\n",(0,s.jsx)(n.li,{children:"The process of converting a token ID to a vector space is called embedding"}),"\n",(0,s.jsxs)(n.li,{children:["How many properties should the vector space capture?","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"We don't know"}),"\n",(0,s.jsxs)(n.li,{children:["Just need to create it large enough to capture ",(0,s.jsx)(n.em,{children:"all"})," of it"]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["GPT2 stores two kinds of embeddings as matrices: one for the tokens (WTE) and one for the position (WPE)","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"The values of the matrices are learned during training"}),"\n",(0,s.jsx)(n.li,{children:"The WPE shape limits the maximum number of tokens that can be used in a prompt for the GPT - which is 1024 in GPT2 and now up to 1million in Gemini 1.5 Pro"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"attention",children:"Attention"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"The groundbreaking feature of the transformer, it gives the ability to transfer some meaning from other token to the current one"}),"\n",(0,s.jsx)(n.li,{children:"Uses 12 sets of matrices called Q (query), K (key) and V (value)"}),"\n",(0,s.jsxs)(n.li,{children:["Each matrix has 64 columns and together they form the attention matrix for each token","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"3 x 12 x 64 columns for each token"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["Temperature","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:'Changes how the softmax function selects the next token, can increase the "creativeness"'}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.li,{}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,i.a)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},11151:(e,n,t)=>{t.d(n,{Z:()=>a,a:()=>o});var s=t(67294);const i={},r=s.createContext(i);function o(e){const n=s.useContext(r);return s.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:o(e.components),s.createElement(r.Provider,{value:n},e.children)}}}]);