"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[13449],{28453:(e,n,i)=>{i.d(n,{R:()=>t,x:()=>a});var s=i(96540);const r={},l=s.createContext(r);function t(e){const n=s.useContext(l);return s.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:t(e.components),s.createElement(l.Provider,{value:n},e.children)}},98775:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>c,contentTitle:()=>a,default:()=>h,frontMatter:()=>t,metadata:()=>s,toc:()=>o});const s=JSON.parse('{"id":"slip-box/reference-notes/Data Science Cheat Sheet","title":"Data Science Cheat Sheet","description":"Inspired by this https://python-data-science.readthedocs.io/en/latest/_images/architecture.png","source":"@site/docs/slip-box/reference-notes/Data Science Cheat Sheet.md","sourceDirName":"slip-box/reference-notes","slug":"/slip-box/reference-notes/Data Science Cheat Sheet","permalink":"/docs/slip-box/reference-notes/Data Science Cheat Sheet","draft":false,"unlisted":false,"editUrl":"https://github.com/paulliwali/paulliwali.github.io/tree/main/docs/docs/slip-box/reference-notes/Data Science Cheat Sheet.md","tags":[{"inline":true,"label":"cheatsheet","permalink":"/docs/tags/cheatsheet"},{"inline":true,"label":"data-science","permalink":"/docs/tags/data-science"},{"inline":true,"label":"machine-learning","permalink":"/docs/tags/machine-learning"}],"version":"current","frontMatter":{"tags":["cheatsheet","data-science","machine-learning"]},"sidebar":"tutorialSidebar","previous":{"title":"Data Engineering Learning","permalink":"/docs/slip-box/reference-notes/Data Engineering Learning"},"next":{"title":"Data Science Learning","permalink":"/docs/slip-box/reference-notes/Data Science Learning"}}');var r=i(74848),l=i(28453);const t={tags:["cheatsheet","data-science","machine-learning"]},a="MLOps flow",c={},o=[];function d(e){const n={a:"a",blockquote:"blockquote",h1:"h1",header:"header",li:"li",ol:"ol",p:"p",ul:"ul",...(0,l.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsxs)(n.blockquote,{children:["\n",(0,r.jsxs)(n.p,{children:["Inspired by this ",(0,r.jsx)(n.a,{href:"https://python-data-science.readthedocs.io/en/latest/_images/architecture.png",children:"https://python-data-science.readthedocs.io/en/latest/_images/architecture.png"})]}),"\n"]}),"\n",(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"mlops-flow",children:"MLOps flow"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Structure: DESIGN + MODEL DEVELOPMENT + OPERATIONS"}),"\n",(0,r.jsx)(n.li,{children:"Design: requirements engineering, use case prioritization, data availability check"}),"\n",(0,r.jsx)(n.li,{children:"Model development: data engineering, model engineering, testing and validation"}),"\n",(0,r.jsx)(n.li,{children:"Operations: model deployment, CI/CD pipelines, monitoring and alerting"}),"\n"]}),"\n",(0,r.jsx)(n.h1,{id:"model-design",children:"Model Design"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["Exploratory Data Analysis","\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"[[Descriptive Statistics]]"}),"\n",(0,r.jsx)(n.li,{children:"[[Distributions]]"}),"\n",(0,r.jsxs)(n.li,{children:["Boxplots - gives descriptions of the data with min/max, IQR and median","\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Gives the 25th percentile to 75th percentile range which is the inter-quantile-range (IQR)"}),"\n",(0,r.jsx)(n.li,{children:"Also gives the min and max which is defined as Q1 - 1.5 * IQR or Q3 + 1.5 * IQR"}),"\n",(0,r.jsx)(n.li,{children:"Outliers are outside of the min and max range"}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.li,{children:"[[Correlations]]"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["Data Preparation","\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["Feature Preprocessing","\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["Fill missing values or remove the column entirely if too many values are missing","\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Impute with mean or median, interpolation with linear or other methods"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["Remove outliers to ensure robustness of sensitive models","\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"As identified in box plots"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["Encode features from string into integer","\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"sine/cosine transformation to maintain cyclic relationships"}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.li,{children:"[[Split training and testing]] to avoid overfitting of the model the training data and avoid data leakage"}),"\n",(0,r.jsx)(n.li,{children:"[[Feature Transformation Techniques]] to bring the distribution of the feature into a more normal like distribution"}),"\n",(0,r.jsx)(n.li,{children:"[[Feature Scaling Techniques]] to bring features into the same space for model convergence"}),"\n",(0,r.jsx)(n.li,{children:"[[Feature Engineering]] to create powerful features that is more informative for the model"}),"\n",(0,r.jsx)(n.li,{children:"[[Class balancing]] to help models predict rare occurrences"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["Model Selection","\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["[[Supervised Learning]]","\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["Regression","\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"[[Linear Regression Model]]"}),"\n",(0,r.jsx)(n.li,{children:"Polynomial regression model"}),"\n",(0,r.jsx)(n.li,{children:"[[Quantile Regression]]"}),"\n",(0,r.jsx)(n.li,{children:"[[Tree Regressors]]"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["Classification","\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"[[KNN]]"}),"\n",(0,r.jsx)(n.li,{children:"[[Naive Bayes]]"}),"\n",(0,r.jsx)(n.li,{children:"[[Support Vector Machines]]"}),"\n",(0,r.jsx)(n.li,{children:"[[Logistic Regression]]"}),"\n",(0,r.jsx)(n.li,{children:"[[Decision Trees]]"}),"\n",(0,r.jsx)(n.li,{children:"[[Tree Ensembles]]"}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["Unsupervised Learning","\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"[[Kernel Regression]]"}),"\n",(0,r.jsx)(n.li,{children:"[[k-means]]"}),"\n",(0,r.jsx)(n.li,{children:"[[Gaussian Mixture Models]]"}),"\n",(0,r.jsx)(n.li,{children:"[[DBSCAN]]"}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.li,{children:"Semi-supervised Learning"}),"\n",(0,r.jsx)(n.li,{children:"Reinforcement Learning"}),"\n",(0,r.jsx)(n.li,{children:"[[Recommendation Systems]]"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["Model Fit","\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["Loss Functions","\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"L1 loss function (sum of the absolute errors)"}),"\n",(0,r.jsx)(n.li,{children:"L2 loss function (sum of the squared errors)"}),"\n",(0,r.jsx)(n.li,{children:"Pinball loss function"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["Hyperparameter tuning result in change in model performance and combined with cross-validation techniques we can find the best set of hyperparameters","\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Grid Search"}),"\n",(0,r.jsx)(n.li,{children:"Random Search"}),"\n",(0,r.jsx)(n.li,{children:"Bayesian Optimization"}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["Model Evaluation","\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Commonly used [[Error metrics]] to evaluate regression type models"}),"\n",(0,r.jsxs)(n.li,{children:["Bias-variance tradeoff","\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"The best predictive model is one that has good generalization ability which is able to predict accurately to new and previously unseen data"}),"\n",(0,r.jsx)(n.li,{children:"high bias can lead to okay performance but too general -> under-fit"}),"\n",(0,r.jsx)(n.li,{children:"high variance can lead to low errors with existing data but not necessarily with new data -> overfit"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["[[Regularization Techniques]]","\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"L1 regularization ([[Lasso Regression]]) which reduces the coefficient values"}),"\n",(0,r.jsx)(n.li,{children:"L2 regularization ([[Ridge Regression]]) which penalizes higher powers"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["Model fit vs complexity","\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"AIC"}),"\n",(0,r.jsx)(n.li,{children:"BIC"}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.li,{children:"SHAPLY"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["Model Score","\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Score on unseen data as the true evaluation of the model"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["Serialization","\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Pickling of the entire process of fitted data preparation and fitted model parameters"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.h1,{id:"inference-testing",children:"Inference testing"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Using sample data to make conclusions about a population parameter"}),"\n",(0,r.jsx)(n.li,{children:"[[Frequentist Inference Testing]]"}),"\n",(0,r.jsx)(n.li,{children:"[[Bayesian Inference Testing]]"}),"\n"]}),"\n",(0,r.jsx)(n.h1,{id:"causal-inference",children:"Causal Inference"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Establishing cause-and-effect relationships. It aims to determine whether a change in one variable (the cause or treatment) leads to a change in another variable (the effect or outcome)"}),"\n",(0,r.jsx)(n.li,{children:"[[AB Testing]]"}),"\n",(0,r.jsxs)(n.li,{children:["Difference in difference","\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Fit a regression to the two groups to the observed metric and the coefficient estimated is the difference"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.h1,{id:"mathematics",children:"Mathematics"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"[[Bayes Theorem]]"}),"\n",(0,r.jsx)(n.li,{children:"[[Linear Algebra]]"}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,l.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(d,{...e})}):d(e)}}}]);