"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[14401],{10888:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>o,contentTitle:()=>l,default:()=>h,frontMatter:()=>a,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"slip-box/reference-notes/Supervised Machine Learning - Regression and Classification","title":"Week 1: Introduction to Machine Learning","description":"Supervised Learning","source":"@site/docs/slip-box/reference-notes/Supervised Machine Learning - Regression and Classification.md","sourceDirName":"slip-box/reference-notes","slug":"/slip-box/reference-notes/Supervised Machine Learning - Regression and Classification","permalink":"/docs/slip-box/reference-notes/Supervised Machine Learning - Regression and Classification","draft":false,"unlisted":false,"editUrl":"https://github.com/paulliwali/paulliwali.github.io/tree/main/docs/docs/slip-box/reference-notes/Supervised Machine Learning - Regression and Classification.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Studying Studies","permalink":"/docs/slip-box/reference-notes/Studying Studies"},"next":{"title":"Support Vector Machines","permalink":"/docs/slip-box/reference-notes/Support Vector Machines"}}');var t=i(74848),r=i(28453);const a={},l="Week 1: Introduction to Machine Learning",o={},c=[{value:"Supervised Learning",id:"supervised-learning",level:2},{value:"Unsupervised Learning",id:"unsupervised-learning",level:2},{value:"[[Linear Regression Model]]",id:"linear-regression-model",level:2},{value:"Multiple Linear Regression",id:"multiple-linear-regression",level:2},{value:"Gradient descent in practice",id:"gradient-descent-in-practice",level:2}];function d(e){const n={annotation:"annotation",code:"code",em:"em",h1:"h1",h2:"h2",header:"header",li:"li",math:"math",mi:"mi",mo:"mo",mrow:"mrow",p:"p",semantics:"semantics",span:"span",ul:"ul",...(0,r.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"week-1-introduction-to-machine-learning",children:"Week 1: Introduction to Machine Learning"})}),"\n",(0,t.jsx)(n.h2,{id:"supervised-learning",children:"Supervised Learning"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["The most common types of algorithms being used today","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"learns from 'input' to 'output label'"}),"\n",(0,t.jsx)(n.li,{children:"use cases: spam filtering, speech recognition, machine translation, online advertising, self driving"}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:['Gives the algorithm datasets to learn from labelled "correct answers"',"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.em,{children:"Regression modeling"})," is to predict a range of numbers"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.em,{children:"Classification modeling"})," is to predict a limited set of class/category"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"unsupervised-learning",children:"Unsupervised Learning"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["Learns from unlabelled output data to find patterns or interesting properties","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"learns from 'input' only as there are no 'output label'"}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.li,{children:"Anomaly detection"}),"\n",(0,t.jsx)(n.li,{children:"Clustering"}),"\n",(0,t.jsx)(n.li,{children:"Dimensionality reduction"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"linear-regression-model",children:"[[Linear Regression Model]]"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["Fitting a ",(0,t.jsx)(n.em,{children:"straight line"})," to a dataset","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"produce a function that produces a y-hat (estimated y)"}),"\n",(0,t.jsxs)(n.li,{children:["formulated as a ",(0,t.jsx)(n.code,{children:"wx+b"})," since it is a linear function"]}),"\n",(0,t.jsx)(n.li,{children:"to train/adjust the unknown parameters/coefficients/weights of the function we need to construct a cost function"}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["Cost function is a way to quantitatively evaluate the goodness of fit of the function","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsxs)(n.span,{className:"katex",children:[(0,t.jsx)(n.span,{className:"katex-mathml",children:(0,t.jsx)(n.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,t.jsxs)(n.semantics,{children:[(0,t.jsxs)(n.mrow,{children:[(0,t.jsx)(n.mi,{children:"J"}),(0,t.jsx)(n.mo,{stretchy:"false",children:"("}),(0,t.jsx)(n.mi,{children:"w"}),(0,t.jsx)(n.mo,{separator:"true",children:","}),(0,t.jsx)(n.mi,{children:"b"}),(0,t.jsx)(n.mo,{stretchy:"false",children:")"}),(0,t.jsx)(n.mo,{children:"="}),(0,t.jsx)(n.mi,{children:"r"}),(0,t.jsx)(n.mi,{children:"m"}),(0,t.jsx)(n.mi,{children:"s"}),(0,t.jsx)(n.mi,{children:"e"}),(0,t.jsx)(n.mi,{mathvariant:"normal",children:"/"}),(0,t.jsx)(n.mi,{children:"m"}),(0,t.jsx)(n.mi,{children:"s"}),(0,t.jsx)(n.mi,{children:"e"})]}),(0,t.jsx)(n.annotation,{encoding:"application/x-tex",children:"J(w, b) = rmse/mse"})]})})}),(0,t.jsxs)(n.span,{className:"katex-html","aria-hidden":"true",children:[(0,t.jsxs)(n.span,{className:"base",children:[(0,t.jsx)(n.span,{className:"strut",style:{height:"1em",verticalAlign:"-0.25em"}}),(0,t.jsx)(n.span,{className:"mord mathnormal",style:{marginRight:"0.09618em"},children:"J"}),(0,t.jsx)(n.span,{className:"mopen",children:"("}),(0,t.jsx)(n.span,{className:"mord mathnormal",style:{marginRight:"0.02691em"},children:"w"}),(0,t.jsx)(n.span,{className:"mpunct",children:","}),(0,t.jsx)(n.span,{className:"mspace",style:{marginRight:"0.1667em"}}),(0,t.jsx)(n.span,{className:"mord mathnormal",children:"b"}),(0,t.jsx)(n.span,{className:"mclose",children:")"}),(0,t.jsx)(n.span,{className:"mspace",style:{marginRight:"0.2778em"}}),(0,t.jsx)(n.span,{className:"mrel",children:"="}),(0,t.jsx)(n.span,{className:"mspace",style:{marginRight:"0.2778em"}})]}),(0,t.jsxs)(n.span,{className:"base",children:[(0,t.jsx)(n.span,{className:"strut",style:{height:"1em",verticalAlign:"-0.25em"}}),(0,t.jsx)(n.span,{className:"mord mathnormal",style:{marginRight:"0.02778em"},children:"r"}),(0,t.jsx)(n.span,{className:"mord mathnormal",children:"m"}),(0,t.jsx)(n.span,{className:"mord mathnormal",children:"se"}),(0,t.jsx)(n.span,{className:"mord",children:"/"}),(0,t.jsx)(n.span,{className:"mord mathnormal",children:"m"}),(0,t.jsx)(n.span,{className:"mord mathnormal",children:"se"})]})]})]})}),"\n",(0,t.jsx)(n.li,{children:"Can be minimized with [[gradient descent]] to find the optimal parameters"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h1,{id:"week-2-regression-with-multiple-input-variables",children:"Week 2: Regression with multiple input variables"}),"\n",(0,t.jsx)(n.h2,{id:"multiple-linear-regression",children:"Multiple Linear Regression"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["Using multiple features to estimate one response variable","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Different from multivariate regression which estimates multiple response variable"}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["Vectorization with ",(0,t.jsx)(n.code,{children:"numpy"})," is used to speed up the algorithm of the dot product between weights and features","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["This is done by doing the operation ",(0,t.jsx)(n.em,{children:"in parallel"})]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.li,{children:"Gradient descent works similarly but instead of taking the derivative of the cost function with respect to one feature's weight and updating one weight -> you update all the weights by taking partial derivatives of the cost function with each weight"}),"\n",(0,t.jsxs)(n.li,{children:["Just a note that an alternative solution to the linear regression is the [[normal equation]]","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"It only works for linear regression because it is a closed form solution to this"}),"\n",(0,t.jsx)(n.li,{children:"So it is not generalized solution (unlike gradient descent) and it could be slower if the features are much higher"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"gradient-descent-in-practice",children:"Gradient descent in practice"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["To make gradient descent more efficient, one should feature scale with some [[Feature Scaling Techniques]]","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"This will make the cost function contour to be evenly spaced and not be susceptible to jumping"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h1,{id:"week-3",children:"Week 3"}),"\n",(0,t.jsx)(n.p,{children:"#machine-learning #course"})]})}function h(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},28453:(e,n,i)=>{i.d(n,{R:()=>a,x:()=>l});var s=i(96540);const t={},r=s.createContext(t);function a(e){const n=s.useContext(r);return s.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:a(e.components),s.createElement(r.Provider,{value:n},e.children)}}}]);